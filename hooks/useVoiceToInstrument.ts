'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import * as Tone from 'tone';
import { NoteData } from '@/types/note';
import { OutputInstrument, ConversionState, INITIAL_CONVERSION_STATE } from '@/types/instrument';
import { getSharedAudioContext } from '@/utils/sharedAudioContext';

/**
 * ========================================
 * [ACTIVE] Tone.js ê¸°ë°˜ í´ë°± ì¬ìƒ
 * ========================================
 * - PolySynthë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒí‘œë¥¼ ì•…ê¸° ì†Œë¦¬ë¡œ ì¬ìƒ
 * - ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë³€í™˜ì€ ì—†ìŒ (í´ë°± ëª¨ë“œ)
 */

/**
 * ========================================
 * [TODO] Magenta.js êµ¬í˜„ (ë¯¸ë˜)
 * ========================================
 * ê³„íš:
 * 1. @magenta/music íŒ¨í‚¤ì§€ ì‚¬ìš©
 * 2. DDSP ëª¨ë¸ ë¡œë“œ (piano/guitar ìŒìƒ‰ ë³€í™˜)
 * 3. ì˜¤ë””ì˜¤ â†’ MIDI ë³€í™˜
 * 4. MIDI â†’ ì•…ê¸° ìŒìƒ‰ ë³€í™˜
 *
 * ì°¸ê³ :
 * - https://github.com/magenta/magenta-js
 * - https://magenta.tensorflow.org/ddsp
 * ========================================
 */

interface UseVoiceToInstrumentReturn {
  conversionState: ConversionState;
  loadModel: (instrument: OutputInstrument) => Promise<boolean>;
  convertAudio: (audioBlob: Blob) => Promise<Blob | null>;
  playNotesAsFallback: (notes: NoteData[], bpm: number, startTime?: number) => Promise<void>;
  stopFallbackPlayback: () => void;
  previewNote: (pitch: string, duration?: number) => Promise<void>;
  cleanup: () => void;
  isModelSupported: () => boolean;
}

/**
 * Synth ìƒì„± í—¬í¼ í•¨ìˆ˜
 */
function createSynth(instrument: OutputInstrument): Tone.PolySynth | null {
  if (instrument === 'piano') {
    return new Tone.PolySynth(Tone.Synth, {
      oscillator: { type: 'triangle', partialCount: 3 },
      envelope: { attack: 0.005, decay: 0.2, sustain: 0.3, release: 1 }
    }).toDestination();
  } else if (instrument === 'guitar') {
    return new Tone.PolySynth(Tone.Synth, {
      oscillator: { type: 'sawtooth', partialCount: 8 },
      envelope: { attack: 0.01, decay: 0.3, sustain: 0.5, release: 0.8 }
    }).toDestination();
  }
  return null;
}

export function useVoiceToInstrument(): UseVoiceToInstrumentReturn {
  const [conversionState, setConversionState] = useState<ConversionState>(INITIAL_CONVERSION_STATE);
  const synthRef = useRef<Tone.PolySynth | null>(null);
  const currentInstrumentRef = useRef<OutputInstrument | null>(null);
  const contextInitialized = useRef(false);

  /**
   * Tone.jsë¥¼ ê³µìœ  AudioContextì— ì—°ê²° (íƒ€ì´ë° ë™ê¸°í™”)
   */
  useEffect(() => {
    if (typeof window === 'undefined' || contextInitialized.current) return;

    try {
      const sharedContext = getSharedAudioContext();
      Tone.setContext(sharedContext);
      contextInitialized.current = true;
      console.log('ğŸ¹ [Tone.js] ê³µìœ  AudioContextì— ì—°ê²°ë¨ - íƒ€ì´ë° ë™ê¸°í™” ì™„ë£Œ');
    } catch (error) {
      console.error('ğŸ¹ [Tone.js] AudioContext ì—°ê²° ì‹¤íŒ¨:', error);
    }
  }, []);

  /**
   * ë¸Œë¼ìš°ì € ì˜¤ë””ì˜¤ ì§€ì› í™•ì¸
   */
  const isModelSupported = useCallback((): boolean => {
    return typeof window !== 'undefined' && typeof AudioContext !== 'undefined';
  }, []);

  /**
   * Tone.js ì‹ ë””ì‚¬ì´ì € ë¡œë“œ
   */
  const loadModel = useCallback(async (instrument: OutputInstrument): Promise<boolean> => {
    if (instrument === 'raw') {
      return true;
    }

    try {
      console.log(`ğŸ¹ [Tone.js] ${instrument} ì‹ ë””ì‚¬ì´ì € ë¡œë“œ ì¤‘...`);

      synthRef.current = createSynth(instrument);
      currentInstrumentRef.current = instrument;

      console.log(`ğŸ¹ [Tone.js] ${instrument} ì‹ ë””ì‚¬ì´ì € ë¡œë“œ ì™„ë£Œ`);
      return true;

    } catch (error) {
      console.error('ğŸ¹ [Tone.js] ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:', error);
      return false;
    }
  }, []);

  /**
   * ì˜¤ë””ì˜¤ ë³€í™˜ (í´ë°± ëª¨ë“œì—ì„œëŠ” ë¶ˆí•„ìš”)
   */
  const convertAudio = useCallback(async (audioBlob: Blob): Promise<Blob | null> => {
    console.log('ğŸ¹ [Tone.js] í´ë°± ëª¨ë“œ - ì˜¤ë””ì˜¤ ë³€í™˜ ì—†ìŒ');
    return null;
  }, []);

  /**
   * duration ë¬¸ìì—´ì„ beat ë‹¨ìœ„ë¡œ ë³€í™˜
   * "w" = 4, "h" = 2, "q" = 1, "8" = 0.5, "16" = 0.25
   */
  const durationToBeats = (duration: string): number => {
    const durationMap: Record<string, number> = {
      'w': 4,   // whole note
      'h': 2,   // half note
      'q': 1,   // quarter note
      '8': 0.5, // eighth note
      '16': 0.25 // sixteenth note
    };
    return durationMap[duration] || 1; // ê¸°ë³¸ê°’ì€ quarter note
  };

  /**
   * ìŒí‘œë¥¼ Tone.jsë¡œ ì¬ìƒ
   *
   * ìˆ˜ì§ì„  ë™ê¸°í™”:
   * - ìˆ˜ì§ì„ ì€ webAudio.currentTime ê¸°ì¤€ìœ¼ë¡œ í‘œì‹œë¨
   * - Tone.jsëŠ” Tone.now() ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì•½ë¨
   * - ë‘ í´ëŸ­ ê°„ ì§€ì—°ì´ ìˆìœ¼ë¯€ë¡œ SYNC_DELAY_SECë¥¼ ì¶”ê°€í•˜ì—¬ ë³´ì •
   */
  const playNotesAsFallback = useCallback(async (
    notes: NoteData[],
    bpm: number,
    startTime: number = 0
  ): Promise<void> => {
    if (!synthRef.current) {
      console.warn('ğŸ¹ [Tone.js] ì‹ ë””ì‚¬ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ');
      return;
    }

    try {
      await Tone.start();

      const secondsPerBeat = 60 / bpm;
      const now = Tone.now();

      // ìˆ˜ì§ì„ ê³¼ Tone.js ë™ê¸°í™” (0: ì •í™•í•œ ë¹„íŠ¸ì— ì¬ìƒ)
      const SYNC_DELAY_SLOTS = 0;
      const SYNC_DELAY_SEC = (SYNC_DELAY_SLOTS / 4) * secondsPerBeat;

      // startTime(ì´ˆ)ì„ beatìœ¼ë¡œ ë³€í™˜
      const startBeat = startTime / secondsPerBeat;

      let scheduledCount = 0;
      let skippedCount = 0;

      // ğŸ” ì²« 5ê°œ ìŒí‘œì˜ ìƒì„¸ íƒ€ì´ë° ë””ë²„ê¹…
      console.log('ğŸ” [DEBUG] ì²« 5ê°œ ìŒí‘œ íƒ€ì´ë°:', {
        now: now.toFixed(3),
        startTime: startTime.toFixed(3),
        startBeat: startBeat.toFixed(3),
        secondsPerBeat: secondsPerBeat.toFixed(3),
        syncDelay: SYNC_DELAY_SEC.toFixed(3)
      });

      notes.slice(0, 5).forEach((note, i) => {
        const triggerTime = now + (note.beat - startBeat) * secondsPerBeat + SYNC_DELAY_SEC;
        const delay = triggerTime - now;
        console.log(`  [${i}] ${note.pitch} (beat=${note.beat.toFixed(2)}, measure=${note.measureIndex}):`, {
          triggerTime: triggerTime.toFixed(3),
          delay: delay.toFixed(3) + 's',
          willSkip: delay < 0
        });
      });

      notes.forEach(note => {
        // ìŒí‘œì˜ íŠ¸ë¦¬ê±° ì‹œê°„ ê³„ì‚° (í˜„ì¬ ì¬ìƒ ìœ„ì¹˜ ê¸°ì¤€ + ë™ê¸°í™” ì§€ì—°)
        const triggerTime = now + (note.beat - startBeat) * secondsPerBeat + SYNC_DELAY_SEC;
        const delay = triggerTime - now;

        // ë„ˆë¬´ ì§€ë‚œ ìŒí‘œëŠ” ìŠ¤í‚µ (1ë¹„íŠ¸ ì´ìƒ ì§€ë‚¨)
        if (delay < -secondsPerBeat) {
          skippedCount++;
          return;
        }

        const durationInBeats = durationToBeats(note.duration);
        const durationInSeconds = durationInBeats * secondsPerBeat;

        // ì•½ê°„ ì§€ë‚œ ìŒí‘œ(SYNC_DELAYë¡œ ì¸í•œ)ëŠ” ì¦‰ì‹œ ì¬ìƒ
        const actualTriggerTime = triggerTime < now ? now + 0.01 : triggerTime;

        synthRef.current?.triggerAttackRelease(
          note.pitch,
          durationInSeconds,
          actualTriggerTime
        );
        scheduledCount++;
      });

      console.log('ğŸ¹ [Tone.js] ìŒí‘œ ì¬ìƒ ì‹œì‘', {
        totalNotes: notes.length,
        scheduled: scheduledCount,
        skipped: skippedCount,
        bpm,
        startTime: startTime.toFixed(2) + 's',
        startBeat: startBeat.toFixed(1),
        syncDelay: SYNC_DELAY_SEC.toFixed(3) + 's'
      });

    } catch (error) {
      console.error('ğŸ¹ [Tone.js] ì¬ìƒ ì¤‘ ì—ëŸ¬:', error);
    }
  }, []);

  /**
   * ì¬ìƒ ì¤‘ì§€
   * - releaseAll()ì€ í˜„ì¬ ì¬ìƒ ì¤‘ì¸ ìŒí‘œë§Œ ì¤‘ì§€
   * - ë¯¸ë˜ì— ì˜ˆì•½ëœ ìŒí‘œë¥¼ ì·¨ì†Œí•˜ë ¤ë©´ synthë¥¼ disposeí•˜ê³  ì¬ìƒì„±í•´ì•¼ í•¨
   */
  const stopFallbackPlayback = useCallback(() => {
    if (synthRef.current) {
      synthRef.current.releaseAll();
      synthRef.current.dispose();

      // ì¦‰ì‹œ ì¬ìƒì„± (ë¯¸ë˜ ì˜ˆì•½ ì´ë²¤íŠ¸ ëª¨ë‘ ì·¨ì†Œë¨)
      if (currentInstrumentRef.current && currentInstrumentRef.current !== 'raw') {
        synthRef.current = createSynth(currentInstrumentRef.current);
        console.log('ğŸ¹ [Tone.js] ì¬ìƒ ì¤‘ì§€ ë° synth ì¬ìƒì„±');
      } else {
        synthRef.current = null;
        console.log('ğŸ¹ [Tone.js] ì¬ìƒ ì¤‘ì§€');
      }
    }
  }, []);

  /**
   * ìŒí‘œ ë¯¸ë¦¬ë“£ê¸° (ì§§ê²Œ ì¬ìƒ)
   */
  const previewNote = useCallback(async (pitch: string, duration: number = 0.3) => {
    if (!synthRef.current) {
      console.warn('ğŸ¹ [Preview] ì‹ ë””ì‚¬ì´ì €ê°€ ë¡œë“œë˜ì§€ ì•ŠìŒ');
      return;
    }

    try {
      // AudioContext í™œì„±í™” í›„ ì¬ìƒ (ì²« í´ë¦­ì—ì„œ í•„ìš”)
      await Tone.start();
      synthRef.current.triggerAttackRelease(pitch, duration);
      console.log(`ğŸ¹ [Preview] ${pitch} ë¯¸ë¦¬ë“£ê¸° (${duration}s)`);
    } catch (error) {
      console.error('ğŸ¹ [Preview] ì¬ìƒ ì¤‘ ì—ëŸ¬:', error);
    }
  }, []);

  /**
   * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   */
  const cleanup = useCallback(() => {
    if (synthRef.current) {
      try {
        synthRef.current.dispose();
      } catch (e) {
        // ë¬´ì‹œ
      }
      synthRef.current = null;
    }

    setConversionState(INITIAL_CONVERSION_STATE);
    currentInstrumentRef.current = null;
  }, []);

  return {
    conversionState,
    loadModel,
    convertAudio,
    playNotesAsFallback,
    stopFallbackPlayback,
    previewNote,
    cleanup,
    isModelSupported,
  };
}

export default useVoiceToInstrument;
