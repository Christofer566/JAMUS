'use client';

import { useState, useRef, useCallback, useEffect } from 'react';

// AudioUrls íƒ€ì… ì •ì˜
export interface AudioUrls {
  intro: string;
  chorus: string;
  outro: string;
}

// í›… ë°˜í™˜ íƒ€ì…
export interface UseWebAudioReturn {
  isLoading: boolean;
  isReady: boolean;
  isPlaying: boolean;
  currentTime: number;
  duration: number;
  loadAudio: (urls: AudioUrls) => Promise<void>;
  play: () => Promise<void>;
  pause: () => void;
  seek: (time: number) => void;
  stop: () => void;
}

// Safari í˜¸í™˜ AudioContext íƒ€ì…
type AudioContextType = AudioContext | typeof window.webkitAudioContext;

// window íƒ€ì… í™•ì¥ (Safariìš©)
declare global {
  interface Window {
    webkitAudioContext: typeof AudioContext;
  }
}

/**
 * Web Audio API ê¸°ë°˜ ì˜¤ë””ì˜¤ ì¬ìƒ í›…
 * intro + chorusÃ—4 + outroë¥¼ í•˜ë‚˜ì˜ ì—°ì† ë²„í¼ë¡œ í•©ì„±í•˜ì—¬ ì¬ìƒ
 */
export function useWebAudio(): UseWebAudioReturn {
  // ìƒíƒœ
  const [isLoading, setIsLoading] = useState(false);
  const [isReady, setIsReady] = useState(false);
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [duration, setDuration] = useState(0);

  // refs
  const audioContextRef = useRef<AudioContext | null>(null);
  const combinedBufferRef = useRef<AudioBuffer | null>(null);
  const sourceNodeRef = useRef<AudioBufferSourceNode | null>(null);
  const startTimeRef = useRef<number>(0); // AudioContext.currentTime at play start
  const pauseOffsetRef = useRef<number>(0); // ì¼ì‹œì •ì§€ ì‹œ ìœ„ì¹˜
  const animationFrameRef = useRef<number | null>(null);
  const isPlayingRef = useRef(false); // isPlayingì˜ ref ë²„ì „ (ì½œë°±ì—ì„œ ì‚¬ìš©)
  const durationRef = useRef(0); // durationì˜ ref ë²„ì „

  // isPlaying ìƒíƒœì™€ ref ë™ê¸°í™”
  useEffect(() => {
    isPlayingRef.current = isPlaying;
  }, [isPlaying]);

  // duration ìƒíƒœì™€ ref ë™ê¸°í™”
  useEffect(() => {
    durationRef.current = duration;
  }, [duration]);

  /**
   * AudioContext ì´ˆê¸°í™” (Safari í´ë°± í¬í•¨)
   */
  const getAudioContext = useCallback((): AudioContext => {
    if (!audioContextRef.current) {
      const AudioContextClass = window.AudioContext || window.webkitAudioContext;
      audioContextRef.current = new AudioContextClass();
    }
    return audioContextRef.current;
  }, []);

  /**
   * URLì—ì„œ AudioBuffer ë¡œë“œ
   */
  const fetchAudioBuffer = useCallback(async (
    context: AudioContext,
    url: string
  ): Promise<AudioBuffer> => {
    const response = await fetch(url);
    if (!response.ok) {
      throw new Error(`Failed to fetch audio: ${url}`);
    }
    const arrayBuffer = await response.arrayBuffer();
    return context.decodeAudioData(arrayBuffer);
  }, []);

  /**
   * ì—¬ëŸ¬ AudioBufferë¥¼ í•˜ë‚˜ë¡œ í•©ì„±
   * intro + chorusÃ—4 + outro
   */
  const combineBuffers = useCallback((
    context: AudioContext,
    intro: AudioBuffer,
    chorus: AudioBuffer,
    outro: AudioBuffer
  ): AudioBuffer => {
    // ì´ ê¸¸ì´ ê³„ì‚°: intro + chorusÃ—4 + outro
    const totalLength = intro.length + (chorus.length * 4) + outro.length;
    const sampleRate = intro.sampleRate;
    const numberOfChannels = Math.max(intro.numberOfChannels, chorus.numberOfChannels, outro.numberOfChannels);

    // ğŸ§ª ë””ë²„ê¹… ë¡œê·¸
    console.log('ğŸµ [combineBuffers] Buffer info:', {
      intro: { length: intro.length, duration: intro.duration.toFixed(2) + 's' },
      chorus: { length: chorus.length, duration: chorus.duration.toFixed(2) + 's' },
      outro: { length: outro.length, duration: outro.duration.toFixed(2) + 's' },
      totalLength,
      expectedTotal: intro.length + (chorus.length * 4) + outro.length,
      numberOfChannels,
      sampleRate,
    });

    // ìƒˆ ë²„í¼ ìƒì„±
    const combined = context.createBuffer(numberOfChannels, totalLength, sampleRate);

    // ê° ì±„ë„ì— ë°ì´í„° ë³µì‚¬
    for (let channel = 0; channel < numberOfChannels; channel++) {
      const outputData = combined.getChannelData(channel);

      // ê° ì±„ë„ë§ˆë‹¤ offsetì„ 0ë¶€í„° ì‹œì‘ (ì´ì „ ë²„ê·¸: offsetì´ ì±„ë„ ê°„ ëˆ„ì ë¨)
      let offset = 0;

      // 1. Intro ë³µì‚¬
      const introData = channel < intro.numberOfChannels
        ? intro.getChannelData(channel)
        : intro.getChannelData(0);
      console.log(`ğŸµ [combineBuffers] Ch${channel} - Intro: offset=${offset}, length=${introData.length}`);
      outputData.set(introData, offset);
      offset += intro.length;

      // 2. Chorus Ã— 4 ë³µì‚¬
      const chorusData = channel < chorus.numberOfChannels
        ? chorus.getChannelData(channel)
        : chorus.getChannelData(0);
      for (let i = 0; i < 4; i++) {
        console.log(`ğŸµ [combineBuffers] Ch${channel} - Chorus[${i}]: offset=${offset}, length=${chorusData.length}`);
        outputData.set(chorusData, offset);
        offset += chorus.length;
      }

      // 3. Outro ë³µì‚¬
      const outroData = channel < outro.numberOfChannels
        ? outro.getChannelData(channel)
        : outro.getChannelData(0);
      console.log(`ğŸµ [combineBuffers] Ch${channel} - Outro: offset=${offset}, length=${outroData.length}`);
      outputData.set(outroData, offset);
      offset += outro.length;

      console.log(`ğŸµ [combineBuffers] Ch${channel} - Final offset=${offset}, totalLength=${totalLength}`);
    }

    return combined;
  }, []);

  /**
   * í˜„ì¬ ì¬ìƒ ì‹œê°„ ì—…ë°ì´íŠ¸ (requestAnimationFrame)
   */
  const updateCurrentTime = useCallback(() => {
    if (!audioContextRef.current || !isPlaying) return;

    const elapsed = audioContextRef.current.currentTime - startTimeRef.current;
    const newTime = pauseOffsetRef.current + elapsed;

    // durationì„ ë„˜ì§€ ì•Šë„ë¡ ì œí•œ
    if (newTime >= duration && duration > 0) {
      setCurrentTime(duration);
      setIsPlaying(false);
      pauseOffsetRef.current = 0;
      return;
    }

    setCurrentTime(newTime);
    animationFrameRef.current = requestAnimationFrame(updateCurrentTime);
  }, [isPlaying, duration]);

  /**
   * ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ë° í•©ì„±
   */
  const loadAudio = useCallback(async (urls: AudioUrls): Promise<void> => {
    console.log('ğŸµ [loadAudio] Starting audio load...', urls);

    setIsLoading(true);
    setIsReady(false);
    setCurrentTime(0);
    setDuration(0);
    pauseOffsetRef.current = 0;
    isPlayingRef.current = false;
    setIsPlaying(false);

    // ì´ì „ ì¬ìƒ ì™„ì „ ì •ì§€ ë° ë²„í¼ ì´ˆê¸°í™”
    if (sourceNodeRef.current) {
      try {
        sourceNodeRef.current.onended = null; // ì½œë°± ì œê±°
        sourceNodeRef.current.stop();
        sourceNodeRef.current.disconnect();
      } catch {
        // ì´ë¯¸ ì •ì§€ë¨
      }
      sourceNodeRef.current = null;
    }

    // ì´ì „ ë²„í¼ ì´ˆê¸°í™” (ì¤‘ìš”: ìƒˆ ê³¡ ë¡œë“œ ì „ ê¸°ì¡´ ë²„í¼ ì œê±°)
    combinedBufferRef.current = null;
    console.log('ğŸµ [loadAudio] ì´ì „ ë²„í¼ ì´ˆê¸°í™” ì™„ë£Œ');

    try {
      // Step 1: AudioContext ì´ˆê¸°í™”
      console.log('ğŸµ [loadAudio] Step 1: Getting AudioContext...');
      const context = getAudioContext();
      console.log('ğŸµ [loadAudio] AudioContext state:', context.state, 'sampleRate:', context.sampleRate);

      // AudioContextê°€ suspended ìƒíƒœì—¬ë„ ì§„í–‰ (íŒŒì¼ ë¡œë“œ/ë””ì½”ë”©ì€ ê°€ëŠ¥)
      // resumeì€ play() ì‹œì ì— ì²˜ë¦¬
      if (context.state === 'suspended') {
        console.log('ğŸµ [loadAudio] AudioContext is suspended - will resume on play()');
      }

      // Step 2: íŒŒì¼ fetch
      console.log('ğŸµ [loadAudio] Step 2: Fetching audio files...');

      let introResponse: Response, chorusResponse: Response, outroResponse: Response;
      try {
        [introResponse, chorusResponse, outroResponse] = await Promise.all([
          fetch(urls.intro),
          fetch(urls.chorus),
          fetch(urls.outro),
        ]);
        console.log('ğŸµ [loadAudio] Fetch results:', {
          intro: { ok: introResponse.ok, status: introResponse.status, size: introResponse.headers.get('content-length') },
          chorus: { ok: chorusResponse.ok, status: chorusResponse.status, size: chorusResponse.headers.get('content-length') },
          outro: { ok: outroResponse.ok, status: outroResponse.status, size: outroResponse.headers.get('content-length') },
        });
      } catch (fetchError) {
        console.error('ğŸ”´ [loadAudio] Fetch failed:', fetchError);
        throw fetchError;
      }

      if (!introResponse.ok || !chorusResponse.ok || !outroResponse.ok) {
        const errorMsg = `Fetch failed: intro=${introResponse.status}, chorus=${chorusResponse.status}, outro=${outroResponse.status}`;
        console.error('ğŸ”´ [loadAudio]', errorMsg);
        throw new Error(errorMsg);
      }

      // Step 3: ArrayBuffer ë³€í™˜
      console.log('ğŸµ [loadAudio] Step 3: Converting to ArrayBuffer...');
      let introArrayBuffer: ArrayBuffer, chorusArrayBuffer: ArrayBuffer, outroArrayBuffer: ArrayBuffer;
      try {
        [introArrayBuffer, chorusArrayBuffer, outroArrayBuffer] = await Promise.all([
          introResponse.arrayBuffer(),
          chorusResponse.arrayBuffer(),
          outroResponse.arrayBuffer(),
        ]);
        console.log('ğŸµ [loadAudio] ArrayBuffer sizes:', {
          intro: (introArrayBuffer.byteLength / 1024).toFixed(1) + 'KB',
          chorus: (chorusArrayBuffer.byteLength / 1024).toFixed(1) + 'KB',
          outro: (outroArrayBuffer.byteLength / 1024).toFixed(1) + 'KB',
        });
      } catch (bufferError) {
        console.error('ğŸ”´ [loadAudio] ArrayBuffer conversion failed:', bufferError);
        throw bufferError;
      }

      // Step 4: AudioBuffer ë””ì½”ë”©
      console.log('ğŸµ [loadAudio] Step 4: Decoding audio data...');
      let introBuffer: AudioBuffer, chorusBuffer: AudioBuffer, outroBuffer: AudioBuffer;
      try {
        [introBuffer, chorusBuffer, outroBuffer] = await Promise.all([
          context.decodeAudioData(introArrayBuffer),
          context.decodeAudioData(chorusArrayBuffer),
          context.decodeAudioData(outroArrayBuffer),
        ]);
        console.log('ğŸµ [loadAudio] Decoded AudioBuffers:', {
          intro: { duration: introBuffer.duration.toFixed(2) + 's', channels: introBuffer.numberOfChannels, sampleRate: introBuffer.sampleRate },
          chorus: { duration: chorusBuffer.duration.toFixed(2) + 's', channels: chorusBuffer.numberOfChannels, sampleRate: chorusBuffer.sampleRate },
          outro: { duration: outroBuffer.duration.toFixed(2) + 's', channels: outroBuffer.numberOfChannels, sampleRate: outroBuffer.sampleRate },
        });
      } catch (decodeError) {
        console.error('ğŸ”´ [loadAudio] decodeAudioData failed:', decodeError);
        throw decodeError;
      }

      // Step 5: ë²„í¼ í•©ì„±
      console.log('ğŸµ [loadAudio] Step 5: Combining buffers (intro + chorusÃ—4 + outro)...');
      let combined: AudioBuffer;
      try {
        combined = combineBuffers(context, introBuffer, chorusBuffer, outroBuffer);
        console.log('ğŸµ [loadAudio] Combined buffer:', {
          duration: combined.duration.toFixed(2) + 's',
          channels: combined.numberOfChannels,
          length: combined.length,
          sampleRate: combined.sampleRate,
        });
      } catch (combineError) {
        console.error('ğŸ”´ [loadAudio] Buffer combining failed:', combineError);
        throw combineError;
      }

      combinedBufferRef.current = combined;

      // duration ì„¤ì • (ì´ˆ ë‹¨ìœ„)
      setDuration(combined.duration);
      setIsReady(true);

      console.log('âœ… [loadAudio] Audio load complete!', {
        intro: introBuffer.duration.toFixed(2) + 's',
        chorus: chorusBuffer.duration.toFixed(2) + 's Ã— 4',
        outro: outroBuffer.duration.toFixed(2) + 's',
        total: combined.duration.toFixed(2) + 's',
      });
    } catch (error) {
      console.error('ğŸ”´ [loadAudio] Failed to load audio:', error);
      if (error instanceof Error) {
        console.error('ğŸ”´ [loadAudio] Error details:', {
          name: error.name,
          message: error.message,
          stack: error.stack,
        });
      }
      setIsReady(false);
    } finally {
      setIsLoading(false);
    }
  }, [getAudioContext, combineBuffers]);

  /**
   * ì¬ìƒ ì‹œì‘ (async - AudioContext resume ëŒ€ê¸°, ref ê¸°ë°˜)
   */
  const play = useCallback(async () => {
    if (!combinedBufferRef.current || !audioContextRef.current) {
      console.warn('ğŸ”´ [play] Audio not ready');
      return;
    }

    const context = audioContextRef.current;
    console.log('ğŸµ [play] Starting playback, AudioContext state:', context.state);

    // AudioContextê°€ suspended ìƒíƒœë©´ resume (ì‚¬ìš©ì ì¸í„°ë™ì…˜ í•„ìš”)
    if (context.state === 'suspended') {
      console.log('ğŸµ [play] Resuming suspended AudioContext...');
      try {
        await context.resume();
        console.log('ğŸµ [play] AudioContext resumed, state:', context.state);
      } catch (error) {
        console.error('ğŸ”´ [play] Failed to resume AudioContext:', error);
        return;
      }
    }

    // ì´ì „ sourceNode ì •ë¦¬
    if (sourceNodeRef.current) {
      try {
        sourceNodeRef.current.onended = null; // ì½œë°± ì œê±°
        sourceNodeRef.current.stop();
        sourceNodeRef.current.disconnect();
      } catch {
        // ì´ë¯¸ ì •ì§€ë¨
      }
      sourceNodeRef.current = null;
    }

    // ìƒˆ sourceNode ìƒì„±
    const source = context.createBufferSource();
    source.buffer = combinedBufferRef.current;
    source.connect(context.destination);

    // ì¬ìƒ ì™„ë£Œ ì‹œ ì²˜ë¦¬ (ref ê¸°ë°˜) - ë°˜ë³µ ì¬ìƒ
    source.onended = () => {
      // refë¡œ í˜„ì¬ ìƒíƒœ í™•ì¸ (í´ë¡œì € ë¬¸ì œ ë°©ì§€)
      if (isPlayingRef.current && sourceNodeRef.current === source) {
        console.log('ğŸµ [play:onended] ì¬ìƒ ì™„ë£Œ â†’ ë°˜ë³µ ì¬ìƒ');

        // ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì¬ìƒ
        pauseOffsetRef.current = 0;
        setCurrentTime(0);

        // ìƒˆ source ìƒì„±í•˜ì—¬ ë°˜ë³µ ì¬ìƒ
        const ctx = audioContextRef.current;
        const buffer = combinedBufferRef.current;
        if (ctx && buffer) {
          const newSource = ctx.createBufferSource();
          newSource.buffer = buffer;
          newSource.connect(ctx.destination);

          // ìƒˆ sourceì—ë„ ê°™ì€ onended í•¸ë“¤ëŸ¬ ì—°ê²° (ì¬ê·€ì  ë°˜ë³µ)
          newSource.onended = source.onended;

          startTimeRef.current = ctx.currentTime;
          newSource.start(0, 0);
          sourceNodeRef.current = newSource;
          console.log('ğŸµ [play:onended] ë°˜ë³µ ì¬ìƒ ì‹œì‘');
        }
      }
    };

    // í˜„ì¬ ìœ„ì¹˜ì—ì„œ ì¬ìƒ ì‹œì‘
    startTimeRef.current = context.currentTime;
    source.start(0, pauseOffsetRef.current);
    sourceNodeRef.current = source;
    console.log('ğŸµ [play] Playback started at offset:', pauseOffsetRef.current.toFixed(2) + 's');

    setIsPlaying(true);
    isPlayingRef.current = true;
  }, []); // ì˜ì¡´ì„± ì œê±° - ref ì‚¬ìš©

  /**
   * ì¼ì‹œì •ì§€ (ë” ì•ˆì „í•˜ê²Œ)
   */
  const pause = useCallback(() => {
    console.log('ğŸµ [pause] í˜¸ì¶œë¨, isPlaying:', isPlayingRef.current);

    if (!audioContextRef.current) {
      console.log('ğŸµ [pause] AudioContext ì—†ìŒ');
      return;
    }

    // í˜„ì¬ ìœ„ì¹˜ ì €ì¥
    if (isPlayingRef.current) {
      const elapsed = audioContextRef.current.currentTime - startTimeRef.current;
      pauseOffsetRef.current += elapsed;
      console.log('ğŸµ [pause] ì €ì¥ëœ ìœ„ì¹˜:', pauseOffsetRef.current.toFixed(2) + 's');
    }

    // sourceNode ì •ì§€
    if (sourceNodeRef.current) {
      try {
        sourceNodeRef.current.onended = null; // ì½œë°± ì œê±°
        sourceNodeRef.current.stop();
        sourceNodeRef.current.disconnect();
      } catch {
        // ì´ë¯¸ ì •ì§€ë¨
      }
      sourceNodeRef.current = null;
    }

    setIsPlaying(false);
    isPlayingRef.current = false;
    console.log('ğŸµ [pause] ì™„ë£Œ');
  }, []);

  /**
   * íŠ¹ì • ìœ„ì¹˜ë¡œ ì´ë™ (ref ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ)
   */
  const seek = useCallback((time: number) => {
    const dur = durationRef.current;
    const clampedTime = Math.max(0, Math.min(time, dur));

    console.log('ğŸµ [seek]', { time, clampedTime, isPlaying: isPlayingRef.current });

    pauseOffsetRef.current = clampedTime;
    setCurrentTime(clampedTime);

    // ì¬ìƒ ì¤‘ì´ë©´ ìƒˆ ìœ„ì¹˜ì—ì„œ ì¬ì‹œì‘
    if (isPlayingRef.current && combinedBufferRef.current && audioContextRef.current) {
      // í˜„ì¬ ì¬ìƒ ì¤‘ì§€
      if (sourceNodeRef.current) {
        try {
          sourceNodeRef.current.onended = null; // ì½œë°± ì œê±° (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
          sourceNodeRef.current.stop();
          sourceNodeRef.current.disconnect();
        } catch {
          // ì´ë¯¸ ì •ì§€ë¨
        }
        sourceNodeRef.current = null;
      }

      // ìƒˆ ìœ„ì¹˜ì—ì„œ ì¬ìƒ
      const context = audioContextRef.current;
      const source = context.createBufferSource();
      source.buffer = combinedBufferRef.current;
      source.connect(context.destination);

      source.onended = () => {
        // refë¡œ í˜„ì¬ ìƒíƒœ í™•ì¸ (í´ë¡œì € ë¬¸ì œ ë°©ì§€) - ë°˜ë³µ ì¬ìƒ
        if (isPlayingRef.current && sourceNodeRef.current === source) {
          console.log('ğŸµ [seek:onended] ì¬ìƒ ì™„ë£Œ â†’ ë°˜ë³µ ì¬ìƒ');

          // ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì¬ìƒ
          pauseOffsetRef.current = 0;
          setCurrentTime(0);

          // ìƒˆ source ìƒì„±í•˜ì—¬ ë°˜ë³µ ì¬ìƒ
          const ctx = audioContextRef.current;
          const buffer = combinedBufferRef.current;
          if (ctx && buffer) {
            const newSource = ctx.createBufferSource();
            newSource.buffer = buffer;
            newSource.connect(ctx.destination);

            // ìƒˆ sourceì—ë„ ê°™ì€ onended í•¸ë“¤ëŸ¬ ì—°ê²° (ì¬ê·€ì  ë°˜ë³µ)
            newSource.onended = source.onended;

            startTimeRef.current = ctx.currentTime;
            newSource.start(0, 0);
            sourceNodeRef.current = newSource;
            console.log('ğŸµ [seek:onended] ë°˜ë³µ ì¬ìƒ ì‹œì‘');
          }
        }
      };

      startTimeRef.current = context.currentTime;
      source.start(0, clampedTime);
      sourceNodeRef.current = source;
    }
  }, []); // ì˜ì¡´ì„± ì—†ìŒ - ref ì‚¬ìš©

  /**
   * ì •ì§€ + ì²˜ìŒìœ¼ë¡œ (ë” ì•ˆì „í•˜ê²Œ)
   */
  const stop = useCallback(() => {
    console.log('ğŸµ [stop] í˜¸ì¶œë¨');

    if (sourceNodeRef.current) {
      try {
        sourceNodeRef.current.onended = null; // ì½œë°± ì œê±°
        sourceNodeRef.current.stop();
        sourceNodeRef.current.disconnect();
      } catch {
        // ì´ë¯¸ ì •ì§€ë¨
      }
      sourceNodeRef.current = null;
    }

    pauseOffsetRef.current = 0;
    setCurrentTime(0);
    setIsPlaying(false);
    isPlayingRef.current = false;
    console.log('ğŸµ [stop] ì™„ë£Œ');
  }, []);

  // currentTime ì—…ë°ì´íŠ¸ effect
  useEffect(() => {
    if (isPlaying) {
      animationFrameRef.current = requestAnimationFrame(updateCurrentTime);
    } else if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [isPlaying, updateCurrentTime]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  useEffect(() => {
    return () => {
      // sourceNode ì •ë¦¬
      if (sourceNodeRef.current) {
        try {
          sourceNodeRef.current.stop();
          sourceNodeRef.current.disconnect();
        } catch {
          // ì´ë¯¸ ì •ì§€ë¨
        }
      }

      // AudioContext ì •ë¦¬
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }

      // animationFrame ì •ë¦¬
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, []);

  return {
    isLoading,
    isReady,
    isPlaying,
    currentTime,
    duration,
    loadAudio,
    play,
    pause,
    seek,
    stop,
  };
}

export default useWebAudio;
