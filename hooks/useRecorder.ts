'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import { getSharedAudioContext } from '@/utils/sharedAudioContext';

// ============================================
// Types
// ============================================
export type RecordingState = 'idle' | 'recording' | 'recorded';
export type PermissionState = 'prompt' | 'granted' | 'denied';

export interface RecordingSegment {
    id: string;
    blob: Blob;
    url: string;
    startTime: number;
    endTime: number;
    startMeasure: number;
    endMeasure: number;
    prerollDuration: number; // blob ì•ë¶€ë¶„ ê±´ë„ˆë›¸ ì‹œê°„ (ì´ˆ)
}

export interface UseRecorderOptions {
    onError?: (error: string) => void;
    onStateChange?: (state: RecordingState) => void;
}

export interface UseRecorderReturn {
    state: RecordingState;
    permissionState: PermissionState;
    segments: RecordingSegment[];
    recordedMeasures: number[];
    isProcessing: boolean;
    isPaused: boolean;
    error: string | null;
    // For save - combined blob of all segments
    audioBlob: Blob | null;
    recordingRange: { startTime: number; endTime: number; startMeasure: number; endMeasure: number } | null;
    requestPermission: () => Promise<boolean>;
    prepareRecording: () => Promise<boolean>; // MediaRecorder ë¯¸ë¦¬ ì‹œì‘ (preroll)
    startRecording: (startTime: number, startMeasure: number) => Promise<boolean>;
    stopRecording: (endTime: number, endMeasure: number) => Promise<void>;
    pauseJamming: () => void;
    resumeJamming: () => void;
    playRecordingsAtTime: (fromTime: number) => Promise<void>;
    pauseRecordings: () => void;
    resetRecording: () => void;
    hasRecordingAt: (time: number) => boolean;
    getOverlappingSegment: (startMeasure: number, endMeasure: number) => RecordingSegment | null;
}

// ============================================
// Helper: Get supported MIME type
// ============================================
function getSupportedMimeType(): string {
    const types = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg'
    ];

    for (const type of types) {
        if (MediaRecorder.isTypeSupported(type)) {
            return type;
        }
    }

    return 'audio/webm';
}

// ============================================
// Helper: Add silence padding to audio
// ============================================
async function addSilencePadding(
    audioBlob: Blob,
    silenceDuration: number
): Promise<Blob> {
    const audioContext = getSharedAudioContext();

    try {
        const arrayBuffer = await audioBlob.arrayBuffer();
        const recordedBuffer = await audioContext.decodeAudioData(arrayBuffer);

        const sampleRate = recordedBuffer.sampleRate;
        const silenceSamples = Math.floor(silenceDuration * sampleRate);
        const totalSamples = silenceSamples + recordedBuffer.length;
        const numberOfChannels = recordedBuffer.numberOfChannels;

        const offlineContext = new OfflineAudioContext(
            numberOfChannels,
            totalSamples,
            sampleRate
        );

        const combinedBuffer = offlineContext.createBuffer(
            numberOfChannels,
            totalSamples,
            sampleRate
        );

        for (let channel = 0; channel < numberOfChannels; channel++) {
            const combinedData = combinedBuffer.getChannelData(channel);
            const recordedData = recordedBuffer.getChannelData(channel);

            for (let i = 0; i < recordedBuffer.length; i++) {
                combinedData[silenceSamples + i] = recordedData[i];
            }
        }

        const source = offlineContext.createBufferSource();
        source.buffer = combinedBuffer;
        source.connect(offlineContext.destination);
        source.start();

        const renderedBuffer = await offlineContext.startRendering();
        const wavBlob = audioBufferToWavBlob(renderedBuffer);

        return wavBlob;
    } finally {
        // ê³µìœ  AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ
    }
}

// ============================================
// Helper: Convert AudioBuffer to WAV Blob
// ============================================
function audioBufferToWavBlob(buffer: AudioBuffer): Blob {
    const numChannels = buffer.numberOfChannels;
    const sampleRate = buffer.sampleRate;
    const format = 1;
    const bitDepth = 16;

    const bytesPerSample = bitDepth / 8;
    const blockAlign = numChannels * bytesPerSample;

    const dataLength = buffer.length * blockAlign;
    const bufferLength = 44 + dataLength;

    const arrayBuffer = new ArrayBuffer(bufferLength);
    const view = new DataView(arrayBuffer);

    const writeString = (offset: number, str: string) => {
        for (let i = 0; i < str.length; i++) {
            view.setUint8(offset + i, str.charCodeAt(i));
        }
    };

    writeString(0, 'RIFF');
    view.setUint32(4, bufferLength - 8, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, format, true);
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * blockAlign, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitDepth, true);
    writeString(36, 'data');
    view.setUint32(40, dataLength, true);

    let offset = 44;
    const channelData: Float32Array[] = [];
    for (let i = 0; i < numChannels; i++) {
        channelData.push(buffer.getChannelData(i));
    }

    for (let i = 0; i < buffer.length; i++) {
        for (let channel = 0; channel < numChannels; channel++) {
            const sample = Math.max(-1, Math.min(1, channelData[channel][i]));
            const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            view.setInt16(offset, intSample, true);
            offset += 2;
        }
    }

    return new Blob([arrayBuffer], { type: 'audio/wav' });
}

// ============================================
// Main Hook
// ============================================
export function useRecorder(options: UseRecorderOptions = {}): UseRecorderReturn {
    const { onError, onStateChange } = options;

    // State
    const [state, setState] = useState<RecordingState>('idle');
    const [permissionState, setPermissionState] = useState<PermissionState>('prompt');
    const [segments, setSegments] = useState<RecordingSegment[]>([]);
    const [isProcessing, setIsProcessing] = useState(false);
    const [isPaused, setIsPaused] = useState(false);
    const [error, setError] = useState<string | null>(null);

    // Refs
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const chunksRef = useRef<Blob[]>([]);
    const pendingRangeRef = useRef<{ startTime: number; startMeasure: number } | null>(null);
    const recordingActualStartRef = useRef<number>(0); // ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ì‹œì  (performance.now)
    const mediaRecorderStartRef = useRef<number>(0); // MediaRecorder.start() í˜¸ì¶œ ì‹œì 
    const prerollDurationRef = useRef<number>(0); // preroll ì‹œê°„ (ì´ˆ)
    const actualRecordingDurationRef = useRef<number>(0); // ì‹¤ì œ ë…¹ìŒ ì‹œê°„ (ì´ˆ, wall-clock)
    const firstChunkTimeRef = useRef<number>(0); // ì²« ë²ˆì§¸ chunk ë„ì°© ì‹œì 
    const chunkCountRef = useRef<number>(0); // chunk ì¹´ìš´íŠ¸

    // Web Audio API ê¸°ë°˜ ì¬ìƒ (ì •í™•í•œ íƒ€ì´ë° ë™ê¸°í™”)
    const audioContextRef = useRef<AudioContext | null>(null);
    const audioBuffersRef = useRef<Map<string, AudioBuffer>>(new Map());
    const sourceNodesRef = useRef<Map<string, AudioBufferSourceNode>>(new Map());
    const gainNodeRef = useRef<GainNode | null>(null);

    // Computed: recorded measures from all segments
    const recordedMeasures = segments.flatMap(seg => {
        const measures: number[] = [];
        for (let m = seg.startMeasure; m <= seg.endMeasure; m++) {
            measures.push(m);
        }
        return measures;
    }).filter((m, i, arr) => arr.indexOf(m) === i); // unique

    // Computed: combined blob for save (first segment for now, can be extended)
    const audioBlob = segments.length > 0 ? segments[0].blob : null;
    const recordingRange = segments.length > 0 ? {
        startTime: Math.min(...segments.map(s => s.startTime)),
        endTime: Math.max(...segments.map(s => s.endTime)),
        startMeasure: Math.min(...segments.map(s => s.startMeasure)),
        endMeasure: Math.max(...segments.map(s => s.endMeasure))
    } : null;

    // State change callback
    useEffect(() => {
        onStateChange?.(state);
    }, [state, onStateChange]);

    // Cleanup on unmount
    useEffect(() => {
        return () => {
            segments.forEach(seg => URL.revokeObjectURL(seg.url));
            // Web Audio API ì •ë¦¬
            sourceNodesRef.current.forEach(source => {
                try {
                    source.stop();
                    source.disconnect();
                } catch {
                    // ì´ë¯¸ ì •ì§€ë¨
                }
            });
            sourceNodesRef.current.clear();
            audioBuffersRef.current.clear();
            // ê³µìœ  AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ - refë§Œ ì´ˆê¸°í™”
            audioContextRef.current = null;
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
            }
        };
    }, []); // eslint-disable-line react-hooks-exhaustive-deps

    // ========================================
    // Check/Request Permission
    // ========================================
    const requestPermission = useCallback(async (): Promise<boolean> => {
        try {
            if (typeof MediaRecorder === 'undefined') {
                const msg = 'ì´ ë¸Œë¼ìš°ì €ëŠ” ë…¹ìŒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤';
                setError(msg);
                onError?.(msg);
                return false;
            }

            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            streamRef.current = stream;
            setPermissionState('granted');
            setError(null);
            return true;
        } catch (err) {
            console.error('Permission denied:', err);
            setPermissionState('denied');
            const msg = 'ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ í—ˆìš©í•´ì£¼ì„¸ìš”';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [onError]);

    // ========================================
    // Get overlapping segment
    // ========================================
    const getOverlappingSegment = useCallback((startMeasure: number, endMeasure: number): RecordingSegment | null => {
        return segments.find(seg => {
            // Check if ranges overlap
            return !(endMeasure < seg.startMeasure || startMeasure > seg.endMeasure);
        }) || null;
    }, [segments]);

    // ========================================
    // Has recording at time
    // ========================================
    const hasRecordingAt = useCallback((time: number): boolean => {
        return segments.some(seg => time >= seg.startTime && time <= seg.endTime);
    }, [segments]);

    // ========================================
    // Prepare Recording (Preroll - MediaRecorder ë¯¸ë¦¬ ì‹œì‘)
    // ì¹´ìš´íŠ¸ë‹¤ìš´ ì „ì— í˜¸ì¶œí•˜ì—¬ MediaRecorder ì´ˆê¸°í™” ì§€ì—° í•´ì†Œ
    // ========================================
    const prepareRecording = useCallback(async (): Promise<boolean> => {
        if (permissionState !== 'granted') {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        if (!streamRef.current) {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        // ì´ë¯¸ ë…¹ìŒ ì¤‘ì´ë©´ ìŠ¤í‚µ
        if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
            console.log('ğŸ¤ MediaRecorder already running');
            return true;
        }

        try {
            chunksRef.current = [];

            const mimeType = getSupportedMimeType();
            console.log('ğŸ¤ [Preroll] Preparing MediaRecorder with MIME type:', mimeType);

            const mediaRecorder = new MediaRecorder(streamRef.current!, { mimeType });
            mediaRecorderRef.current = mediaRecorder;

            // chunk ì¹´ìš´í„° ì´ˆê¸°í™”
            chunkCountRef.current = 0;
            firstChunkTimeRef.current = 0;

            mediaRecorder.ondataavailable = (event) => {
                const now = performance.now();
                chunkCountRef.current++;

                if (event.data.size > 0) {
                    // ì²« ë²ˆì§¸ ìœ íš¨ chunk ì‹œì  ê¸°ë¡
                    if (firstChunkTimeRef.current === 0) {
                        firstChunkTimeRef.current = now;
                        console.log('ğŸ¤ [Chunk] ì²« ë²ˆì§¸ chunk ë„ì°©:', {
                            ì‹œì : now.toFixed(0) + 'ms',
                            MediaRecorderì‹œì‘í›„: ((now - mediaRecorderStartRef.current) / 1000).toFixed(3) + 's',
                            size: event.data.size
                        });
                    }

                    // ì²˜ìŒ 5ê°œ chunkë§Œ ë¡œê¹…
                    if (chunkCountRef.current <= 5) {
                        console.log(`ğŸ¤ [Chunk ${chunkCountRef.current}] size=${event.data.size}, elapsed=${((now - mediaRecorderStartRef.current) / 1000).toFixed(2)}s`);
                    }

                    chunksRef.current.push(event.data);
                }
            };

            // MediaRecorder ì‹œì‘ ì‹œì  ê¸°ë¡ (preroll ê³„ì‚°ìš©)
            mediaRecorderStartRef.current = performance.now();
            prerollDurationRef.current = 0; // ì•„ì§ ì‹¤ì œ ì‹œì‘ ì „

            mediaRecorder.start(100);
            setState('recording');
            setError(null);

            console.log('ğŸ¤ [Preroll] MediaRecorder.start() í˜¸ì¶œ:', {
                timestamp: mediaRecorderStartRef.current.toFixed(0) + 'ms',
                timeslice: '100ms'
            });
            return true;
        } catch (err) {
            console.error('Prepare recording error:', err);
            const msg = 'ë…¹ìŒ ì¤€ë¹„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [permissionState, requestPermission, onError]);

    // ========================================
    // Start Recording (ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆí‚¹)
    // prepareRecording í›„ ì¹´ìš´íŠ¸ë‹¤ìš´ ì™„ë£Œ ì‹œ í˜¸ì¶œ
    // ========================================
    const startRecording = useCallback(async (
        startTime: number,
        startMeasure: number
    ): Promise<boolean> => {
        // MediaRecorderê°€ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆìœ¼ë©´ (prepareRecording í˜¸ì¶œë¨)
        // ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ì‹œì ë§Œ ë§ˆí‚¹
        if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
            const now = performance.now();
            prerollDurationRef.current = (now - mediaRecorderStartRef.current) / 1000; // ms â†’ s
            recordingActualStartRef.current = now;
            pendingRangeRef.current = { startTime, startMeasure };

            // ì²« chunk ë„ì°© ì‹œì ê³¼ ë¹„êµ
            const firstChunkDelay = firstChunkTimeRef.current > 0
                ? ((firstChunkTimeRef.current - mediaRecorderStartRef.current) / 1000).toFixed(3) + 's'
                : 'ì•„ì§ ì—†ìŒ';
            const chunksReceived = chunkCountRef.current;

            console.log('ğŸ¤ [Actual Start] âš ï¸ íƒ€ì´ë° ë¶„ì„:', {
                prepareRecordingí˜¸ì¶œí›„: prerollDurationRef.current.toFixed(3) + 's',
                ì²«ChunkDelay: firstChunkDelay,
                ë°›ì€Chunks: chunksReceived,
                targetMeasure: startMeasure,
                ê²½ê³ : chunksReceived < 10 ? 'âš ï¸ chunkê°€ ì ìŒ - MediaRecorder ì§€ì—° ê°€ëŠ¥ì„±' : 'OK'
            });
            return true;
        }

        // prepareRecordingì´ í˜¸ì¶œë˜ì§€ ì•Šì€ ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        if (permissionState !== 'granted') {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        if (!streamRef.current) {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        try {
            chunksRef.current = [];

            const mimeType = getSupportedMimeType();
            console.log('ğŸ¤ Recording with MIME type:', mimeType);

            const mediaRecorder = new MediaRecorder(streamRef.current!, { mimeType });
            mediaRecorderRef.current = mediaRecorder;

            pendingRangeRef.current = { startTime, startMeasure };

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunksRef.current.push(event.data);
                }
            };

            // ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ì‹œì  ê¸°ë¡ (ë™ê¸°í™” ë””ë²„ê¹…ìš©)
            const startTimestamp = performance.now();
            recordingActualStartRef.current = startTimestamp;
            mediaRecorderStartRef.current = startTimestamp;
            prerollDurationRef.current = 0; // preroll ì—†ìŒ

            mediaRecorder.start(100);
            setState('recording');
            setError(null);

            console.log('ğŸ¤ Recording started (no preroll):', {
                targetTime: startTime,
                measure: startMeasure,
                actualTimestamp: startTimestamp,
                hint: 'ë™ê¸°í™” í…ŒìŠ¤íŠ¸: ë©”íŠ¸ë¡œë†ˆì— ë§ì¶° ì†ë¼‰ì„ ì¹˜ê³ , ì¬ìƒ ì‹œ ë©”íŠ¸ë¡œë†ˆê³¼ ì†ë¼‰ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸'
            });
            return true;
        } catch (err) {
            console.error('Start recording error:', err);
            const msg = 'ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [permissionState, requestPermission, onError]);

    // ========================================
    // Stop Recording
    // ========================================
    const stopRecording = useCallback(async (
        endTime: number,
        endMeasure: number
    ): Promise<void> => {
        if (!mediaRecorderRef.current || state !== 'recording') {
            return;
        }

        return new Promise((resolve) => {
            const mediaRecorder = mediaRecorderRef.current!;

            mediaRecorder.onstop = async () => {
                console.log('ğŸ¤ Recording stopped, processing...');
                setIsProcessing(true);

                try {
                    const rawBlob = new Blob(chunksRef.current, {
                        type: mediaRecorder.mimeType
                    });

                    const startTime = pendingRangeRef.current?.startTime || 0;
                    const startMeasure = pendingRangeRef.current?.startMeasure || 1;

                    // ë…¹ìŒ ì‹œì‘ ì§€ì—° ë³´ì • (MediaRecorder ì´ˆê¸°í™” + ë²„í¼ë§ ì§€ì—°)
                    // ì´ ê°’ì„ ì¡°ì ˆí•˜ì—¬ ë™ê¸°í™”ë¥¼ ë§ì¶¤:
                    // - ì¬ìƒ ì‹œ ë…¹ìŒì´ ë¹ ë¥´ê²Œ ë“¤ë¦¬ë©´: ê°’ì„ ì¤„ì„ (silence padding ì¦ê°€)
                    // - ì¬ìƒ ì‹œ ë…¹ìŒì´ ëŠ¦ê²Œ ë“¤ë¦¬ë©´: ê°’ì„ ëŠ˜ë¦¼ (silence padding ê°ì†Œ)
                    // í…ŒìŠ¤íŠ¸: ë©”íŠ¸ë¡œë†ˆì— ë§ì¶° ì†ë¼‰ ë…¹ìŒ í›„, ì¬ìƒ ì‹œ ë©”íŠ¸ë¡œë†ˆê³¼ ë¹„êµ
                    const RECORDING_LATENCY_COMPENSATION = 0.12; // ë…¹ìŒì´ ëŠ¦ê²Œ ì¬ìƒë˜ì–´ ë³´ì •
                    const adjustedStartTime = Math.max(0, startTime - RECORDING_LATENCY_COMPENSATION);

                    // ì‹¤ì œ ë…¹ìŒ ì‹œê°„ê³¼ ì˜ˆìƒ ì‹œê°„ ë¹„êµ (ë””ë²„ê¹…ìš©)
                    const recordingDuration = performance.now() - recordingActualStartRef.current;
                    const expectedDuration = (endTime - startTime) * 1000; // ms

                    // ì‹¤ì œ ë…¹ìŒ ì‹œê°„ ì €ì¥ (decodeAudioBufferì—ì„œ ì‚¬ìš©)
                    actualRecordingDurationRef.current = recordingDuration / 1000; // ms â†’ s

                    console.log('ğŸ¤ Recording sync debug:', {
                        expectedDuration: `${expectedDuration.toFixed(0)}ms`,
                        actualDuration: `${recordingDuration.toFixed(0)}ms`,
                        difference: `${(recordingDuration - expectedDuration).toFixed(0)}ms`,
                        latencyCompensation: `${RECORDING_LATENCY_COMPENSATION * 1000}ms`,
                        adjustedStartTime: `${adjustedStartTime.toFixed(3)}s`,
                        note: 'ë¬´ìŒ íŒ¨ë”© ì œê±°ë¨ - ìˆœìˆ˜ ë…¹ìŒ ë°ì´í„°ë§Œ ì €ì¥'
                    });
                    // ë¬´ìŒ íŒ¨ë”© ì œê±°: ìˆœìˆ˜ ë…¹ìŒ ë°ì´í„°ë§Œ ì €ì¥
                    // ì¬ìƒ ì‹œ startTime ì˜¤í”„ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë™ê¸°í™”
                    const url = URL.createObjectURL(rawBlob);
                    const segmentId = `seg-${Date.now()}`;

                    const newSegment: RecordingSegment = {
                        id: segmentId,
                        blob: rawBlob,  // ë¬´ìŒ íŒ¨ë”© ì—†ì´ ìˆœìˆ˜ ë…¹ìŒ ë°ì´í„°
                        url,
                        startTime,
                        endTime,
                        startMeasure,
                        endMeasure,
                        prerollDuration: prerollDurationRef.current // blob ì•ë¶€ë¶„ ê±´ë„ˆë›¸ ì‹œê°„
                    };

                    // ë¨¼ì € ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¶”ê°€ (ì´í›„ íŠ¸ë¦¬ë°ì—ì„œ ì—…ë°ì´íŠ¸ë¨)
                    setSegments(prev => {
                        const result: RecordingSegment[] = [];

                        for (const seg of prev) {
                            const overlaps = !(endMeasure < seg.startMeasure || startMeasure > seg.endMeasure);

                            if (!overlaps) {
                                result.push(seg);
                            } else {
                                if (seg.startMeasure < startMeasure) {
                                    const trimmedSeg: RecordingSegment = {
                                        ...seg,
                                        endMeasure: startMeasure - 1,
                                        endTime: startTime
                                    };
                                    console.log('ğŸ¤ Trimming segment', seg.id, 'from', seg.startMeasure, '-', seg.endMeasure, 'to', trimmedSeg.startMeasure, '-', trimmedSeg.endMeasure);
                                    result.push(trimmedSeg);
                                } else if (seg.endMeasure > endMeasure) {
                                    const trimmedSeg: RecordingSegment = {
                                        ...seg,
                                        startMeasure: endMeasure + 1,
                                        startTime: endTime
                                    };
                                    console.log('ğŸ¤ Trimming segment', seg.id, 'from', seg.startMeasure, '-', seg.endMeasure, 'to', trimmedSeg.startMeasure, '-', trimmedSeg.endMeasure);
                                    result.push(trimmedSeg);
                                } else {
                                    console.log('ğŸ¤ Removing completely overlapped segment', seg.id);
                                    URL.revokeObjectURL(seg.url);
                                    const source = sourceNodesRef.current.get(seg.id);
                                    if (source) {
                                        try {
                                            source.stop();
                                            source.disconnect();
                                        } catch { /* ì´ë¯¸ ì •ì§€ë¨ */ }
                                        sourceNodesRef.current.delete(seg.id);
                                    }
                                    audioBuffersRef.current.delete(seg.id);
                                }
                            }
                        }

                        return [...result, newSegment];
                    });

                    // Web Audio API: Blobì„ AudioBufferë¡œ ë””ì½”ë”© ë° íŠ¸ë¦¬ë°
                    // awaití•˜ì—¬ íŠ¸ë¦¬ë°ì´ ì™„ë£Œëœ í›„ 'recorded' ìƒíƒœë¡œ ì „í™˜
                    // íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜
                    let finalBlobSize = rawBlob.size;
                    let finalPrerollDuration = prerollDurationRef.current;

                    await (async () => {
                        try {
                            // AudioContext ìƒì„± (ì—†ìœ¼ë©´)
                            if (!audioContextRef.current) {
                                audioContextRef.current = getSharedAudioContext();
                                gainNodeRef.current = audioContextRef.current.createGain();
                                gainNodeRef.current.connect(audioContextRef.current.destination);
                            }

                            const arrayBuffer = await rawBlob.arrayBuffer();
                            const audioBuffer = await audioContextRef.current.decodeAudioData(arrayBuffer);
                            audioBuffersRef.current.set(segmentId, audioBuffer);

                            // preroll ê³„ì‚°: blob ê¸¸ì´ - ì‹¤ì œ ë…¹ìŒ ì‹œê°„
                            const blobDuration = audioBuffer.duration;
                            const actualRecordingDuration = actualRecordingDurationRef.current;

                            // ========================================
                            // íŠ¸ë¦¬ë° ê¸°ì¤€: ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œê°„ (prepareRecording â†’ startRecording)
                            // ========================================
                            // ì´ì „ ë°©ì‹: blobDuration - actualRecordingDuration + 0.2s (ë¶ˆì•ˆì •, RMS ì˜ì¡´)
                            // ìƒˆ ë°©ì‹: prerollDurationRef.current ì§ì ‘ ì‚¬ìš© (ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œê°„)
                            //
                            // ê²°ê³¼:
                            // - íŠ¸ë¦¬ë° í›„ blob 0ì´ˆ = ë…¹ìŒ ì‹œì‘ ë§ˆë””ì˜ 0ë°•
                            // - ì‚¬ìš©ìê°€ ëŠ¦ê²Œ ì‹œì‘í•˜ë©´ â†’ ì•ë¶€ë¶„ì´ ì‰¼í‘œë¡œ í‘œì‹œë¨
                            // - ì˜ë„ì ì¸ ì‰¼í‘œê°€ ë³´ì¡´ë¨ (ì¼ê´€ëœ ê²°ê³¼)
                            const prerollToTrim = prerollDurationRef.current;

                            // ë””ë²„ê¹…ìš©: ì´ì „ ë°©ì‹ê³¼ ë¹„êµ
                            const oldBasePreroll = Math.max(0, blobDuration - actualRecordingDuration);
                            const oldPrerollToTrim = oldBasePreroll + 0.2;

                            // ì²« chunk delay ì •ë³´ ì¶”ê°€
                    const firstChunkDelay = firstChunkTimeRef.current > 0
                        ? (firstChunkTimeRef.current - mediaRecorderStartRef.current) / 1000
                        : 0;

                    console.log('ğŸ¤ [TIMING DEBUG] íŠ¸ë¦¬ë° ë¶„ì„:', {
                        'blob_duration': blobDuration.toFixed(2) + 's',
                        'actual_recording_duration': actualRecordingDuration.toFixed(2) + 's',
                        'ì¹´ìš´íŠ¸ë‹¤ìš´_ì‹œê°„(ìƒˆë°©ì‹)': prerollToTrim.toFixed(3) + 's',
                        'ê³„ì‚°ê°’(ì´ì „ë°©ì‹)': oldPrerollToTrim.toFixed(3) + 's',
                        'ì°¨ì´': (prerollToTrim - oldPrerollToTrim).toFixed(3) + 's',
                        'ì²«_chunk_delay': firstChunkDelay.toFixed(3) + 's'
                    });

                    // ë¬¸ì œ ì§„ë‹¨
                    if (firstChunkDelay > 0.5) {
                        console.warn('ğŸ¤ âš ï¸ ì²« chunk delayê°€ 500ms ì´ìƒ! MediaRecorder ì´ˆê¸°í™” ì§€ì—°');
                    }
                    if (blobDuration > actualRecordingDuration + 1) {
                        console.warn('ğŸ¤ âš ï¸ blobì´ ì˜ˆìƒë³´ë‹¤ ' + (blobDuration - actualRecordingDuration).toFixed(2) + 's ê¹€ - ì¶”ê°€ ë¬´ìŒ í¬í•¨ ê°€ëŠ¥');
                    }

                    // ì˜¤ë””ì˜¤ ë²„í¼ ì•ë¶€ë¶„ RMS ë¶„ì„ (ë¬´ìŒ êµ¬ê°„ ì°¾ê¸°)
                    const channelData = audioBuffer.getChannelData(0);
                    const sampleRateForAnalysis = audioBuffer.sampleRate;
                    const analyzeSeconds = [0, 1, 2, 3, 4, 5, 6, 7, 8]; // 0~8ì´ˆ ë¶„ì„
                    const rmsResults: string[] = [];

                    for (const sec of analyzeSeconds) {
                        if (sec >= blobDuration) break;
                        const startSample = Math.floor(sec * sampleRateForAnalysis);
                        const endSample = Math.min(startSample + sampleRateForAnalysis, channelData.length);
                        let sumSquares = 0;
                        for (let i = startSample; i < endSample; i++) {
                            sumSquares += channelData[i] * channelData[i];
                        }
                        const rms = Math.sqrt(sumSquares / (endSample - startSample));
                        const status = rms < 0.005 ? 'ğŸ”‡ë¬´ìŒ' : rms < 0.02 ? 'ğŸ”ˆì•½í•¨' : 'ğŸ”Šì •ìƒ';
                        rmsResults.push(`${sec}s:${rms.toFixed(4)}${status}`);
                    }
                    console.log('ğŸ¤ [AUDIO RMS] ì´ˆë³„ RMS ë¶„ì„ (íŠ¸ë¦¬ë° ì „):', rmsResults.join(' | '));

                            // preroll ë¶€ë¶„ì„ ì˜ë¼ë‚¸ ìƒˆ AudioBuffer ìƒì„±
                            console.log('ğŸ¤ [TRIM CHECK] prerollToTrim:', prerollToTrim.toFixed(3) + 's',
                                prerollToTrim > 0.1 ? 'â†’ íŠ¸ë¦¬ë° ì‹¤í–‰' : 'â†’ âš ï¸ íŠ¸ë¦¬ë° ìŠ¤í‚µ (0.1s ì´í•˜)');

                            // ê²½ê³ : prerollì´ ë¹„ì •ìƒì ìœ¼ë¡œ ì§§ìœ¼ë©´ prepareRecording í˜¸ì¶œ ëˆ„ë½ ê°€ëŠ¥ì„±
                            if (prerollToTrim < 1.0 && prerollToTrim > 0) {
                                console.warn('ğŸ¤ âš ï¸ prerollToTrimì´ 1ì´ˆ ë¯¸ë§Œ! prepareRecording í˜¸ì¶œ ì—¬ë¶€ í™•ì¸ í•„ìš”');
                            }

                            if (prerollToTrim > 0.1) {
                                const sampleRate = audioBuffer.sampleRate;
                                const trimSamples = Math.floor(prerollToTrim * sampleRate);
                                const newLength = audioBuffer.length - trimSamples;

                                if (newLength > 0) {
                                    // OfflineAudioContextë¡œ íŠ¸ë¦¬ë°ëœ ë²„í¼ ìƒì„±
                                    const offlineCtx = new OfflineAudioContext(
                                        audioBuffer.numberOfChannels,
                                        newLength,
                                        sampleRate
                                    );

                                    const source = offlineCtx.createBufferSource();
                                    source.buffer = audioBuffer;
                                    source.connect(offlineCtx.destination);
                                    source.start(0, prerollToTrim); // preroll ì´í›„ë¶€í„° ì‹œì‘

                                    const trimmedBuffer = await offlineCtx.startRendering();

                                    // íŠ¸ë¦¬ë°ëœ ë²„í¼ë¡œ êµì²´
                                    audioBuffersRef.current.set(segmentId, trimmedBuffer);

                                    // íŠ¸ë¦¬ë°ëœ AudioBufferë¥¼ Blobìœ¼ë¡œ ë³€í™˜
                                    const trimmedBlob = audioBufferToWavBlob(trimmedBuffer);
                                    const trimmedUrl = URL.createObjectURL(trimmedBlob);

                                    // ìµœì¢… ê°’ ì—…ë°ì´íŠ¸
                                    finalBlobSize = trimmedBlob.size;
                                    finalPrerollDuration = 0;

                                    console.log('ğŸ¤ íŠ¸ë¦¬ë° ì™„ë£Œ:', {
                                        originalDuration: blobDuration.toFixed(2) + 's',
                                        trimmedDuration: trimmedBuffer.duration.toFixed(2) + 's',
                                        removedPreroll: prerollToTrim.toFixed(3) + 's',
                                        originalBlobSize: rawBlob.size,
                                        trimmedBlobSize: trimmedBlob.size
                                    });

                                    // íŠ¸ë¦¬ë° í›„ RMS ë¶„ì„
                                    const trimmedChannelData = trimmedBuffer.getChannelData(0);
                                    const trimmedRmsResults: string[] = [];
                                    for (const sec of [0, 1, 2, 3, 4, 5, 6, 7, 8]) {
                                        if (sec >= trimmedBuffer.duration) break;
                                        const start = Math.floor(sec * sampleRate);
                                        const end = Math.min(start + sampleRate, trimmedChannelData.length);
                                        let sum = 0;
                                        for (let i = start; i < end; i++) {
                                            sum += trimmedChannelData[i] * trimmedChannelData[i];
                                        }
                                        const rms = Math.sqrt(sum / (end - start));
                                        const status = rms < 0.005 ? 'ğŸ”‡ë¬´ìŒ' : rms < 0.02 ? 'ğŸ”ˆì•½í•¨' : 'ğŸ”Šì •ìƒ';
                                        trimmedRmsResults.push(`${sec}s:${rms.toFixed(4)}${status}`);
                                    }
                                    console.log('ğŸ¤ [AUDIO RMS] ì´ˆë³„ RMS ë¶„ì„ (íŠ¸ë¦¬ë° í›„):', trimmedRmsResults.join(' | '));

                                    // segment ì—…ë°ì´íŠ¸: íŠ¸ë¦¬ë°ëœ blob, url, prerollDuration=0
                                    setSegments(prev => prev.map(seg => {
                                        if (seg.id === segmentId) {
                                            // ê¸°ì¡´ URL í•´ì œ
                                            URL.revokeObjectURL(seg.url);
                                            return {
                                                ...seg,
                                                blob: trimmedBlob,
                                                url: trimmedUrl,
                                                prerollDuration: 0
                                            };
                                        }
                                        return seg;
                                    }));
                                }
                            } else {
                                // prerollì´ ê±°ì˜ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ prerollDurationì€ 0ìœ¼ë¡œ
                                finalPrerollDuration = 0;
                                setSegments(prev => prev.map(seg =>
                                    seg.id === segmentId
                                        ? { ...seg, prerollDuration: 0 }
                                        : seg
                                ));
                            }
                        } catch (err) {
                            console.error('ğŸ¤ Failed to decode audio buffer:', err);
                        }
                    })();

                    setState('recorded');

                    console.log('ğŸ¤ Recording complete:', {
                        id: segmentId,
                        duration: endTime - startTime,
                        measures: `${startMeasure}-${endMeasure}`,
                        finalBlobSize,
                        finalPrerollDuration: finalPrerollDuration.toFixed(3) + 's',
                        note: 'preroll=0ì´ë©´ íŠ¸ë¦¬ë° ì™„ë£Œ'
                    });
                } catch (err) {
                    console.error('Processing error:', err);
                    setError('ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                    onError?.('ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                } finally {
                    setIsProcessing(false);
                    pendingRangeRef.current = null;
                    resolve();
                }
            };

            mediaRecorder.stop();
        });
    }, [state, onError]);

    // ========================================
    // Pause Jamming
    // ========================================
    const pauseJamming = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording' && !isPaused) {
            mediaRecorderRef.current.pause();
            setIsPaused(true);
            console.log('ğŸ¤ Recording paused');
        }
    }, [state, isPaused]);

    // ========================================
    // Resume Jamming
    // ========================================
    const resumeJamming = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording' && isPaused) {
            mediaRecorderRef.current.resume();
            setIsPaused(false);
            console.log('ğŸ¤ Recording resumed');
        }
    }, [state, isPaused]);

    // ========================================
    // Play Recordings At Time (Web Audio API ê¸°ë°˜ - ì •í™•í•œ íƒ€ì´ë°)
    // ========================================
    const playRecordingsAtTime = useCallback(async (fromTime: number) => {
        // Find segments that include this time
        const activeSegments = segments.filter(seg =>
            fromTime >= seg.startTime && fromTime <= seg.endTime
        );

        if (activeSegments.length === 0) {
            return;
        }

        // AudioContext ìƒì„± (ì—†ìœ¼ë©´)
        if (!audioContextRef.current) {
            audioContextRef.current = getSharedAudioContext();
            gainNodeRef.current = audioContextRef.current.createGain();
            gainNodeRef.current.connect(audioContextRef.current.destination);
        }

        const context = audioContextRef.current;

        // suspended ìƒíƒœë©´ resume (ë°˜ë“œì‹œ await!)
        if (context.state === 'suspended') {
            console.log('ğŸ¤ [Web Audio] Resuming suspended AudioContext');
            await context.resume();
        }

        console.log('ğŸ¤ [Web Audio] AudioContext state:', context.state, 'gainNode:', gainNodeRef.current?.gain.value);

        // Play each active segment using Web Audio API
        activeSegments.forEach(seg => {
            const audioBuffer = audioBuffersRef.current.get(seg.id);
            if (!audioBuffer) {
                console.warn('ğŸ¤ AudioBuffer not ready for segment:', seg.id);
                return;
            }

            // ê¸°ì¡´ source node ì •ë¦¬
            const existingSource = sourceNodesRef.current.get(seg.id);
            if (existingSource) {
                try {
                    existingSource.stop();
                    existingSource.disconnect();
                } catch {
                    // ì´ë¯¸ ì •ì§€ë¨
                }
            }

            // ë¸”ë¡­ ë‚´ ì˜¤í”„ì…‹ ê³„ì‚°
            // prerollDuration: blob ì•ë¶€ë¶„ ê±´ë„ˆë›¸ ì‹œê°„ (ì¹´ìš´íŠ¸ë‹¤ìš´ ë™ì•ˆ ë…¹ìŒëœ ë¶€ë¶„)
            // fromTime - seg.startTime: ê³¡ ì‹œê°„ ë‚´ ì˜¤í”„ì…‹
            // PLAYBACK_TIMING_OFFSET: ì¬ìƒ íƒ€ì´ë° ë³´ì • (ë…¹ìŒì´ ëŠ¦ê²Œ ë“¤ë¦¬ë©´ + ê°’)
            const PLAYBACK_TIMING_OFFSET = 0.2;
            const offset = seg.prerollDuration + Math.max(0, fromTime - seg.startTime) + PLAYBACK_TIMING_OFFSET;

            // ìƒˆ source node ìƒì„±
            const source = context.createBufferSource();
            source.buffer = audioBuffer;
            if (gainNodeRef.current) {
                source.connect(gainNodeRef.current);
            } else {
                source.connect(context.destination);
            }

            // ì¦‰ì‹œ ì¬ìƒ (Web Audio APIëŠ” ì •í™•í•œ íƒ€ì´ë° ë³´ì¥)
            source.start(0, offset);
            sourceNodesRef.current.set(seg.id, source);

            console.log('ğŸ¤ [Web Audio] ì¬ìƒ ì‹œì‘:', {
                segId: seg.id,
                fromTime,
                segStartTime: seg.startTime,
                prerollDuration: seg.prerollDuration.toFixed(3),
                offset: offset.toFixed(3),
                bufferDuration: audioBuffer.duration.toFixed(2)
            });

            // ì¬ìƒ ì™„ë£Œ ì‹œ ì •ë¦¬
            source.onended = () => {
                sourceNodesRef.current.delete(seg.id);
            };
        });
    }, [segments]);

    // ========================================
    // Pause Recordings (Web Audio API ê¸°ë°˜)
    // ========================================
    const pauseRecordings = useCallback(() => {
        const count = sourceNodesRef.current.size;
        console.log('ğŸ¤ [pauseRecordings] í˜¸ì¶œë¨, í™œì„± ì†ŒìŠ¤:', count);

        sourceNodesRef.current.forEach((source, id) => {
            try {
                source.stop();
                source.disconnect();
                console.log('ğŸ¤ [pauseRecordings] ì†ŒìŠ¤ ì •ì§€:', id);
            } catch (e) {
                console.log('ğŸ¤ [pauseRecordings] ì†ŒìŠ¤ ì´ë¯¸ ì •ì§€ë¨:', id);
            }
        });
        sourceNodesRef.current.clear();
        console.log('ğŸ¤ [pauseRecordings] ì™„ë£Œ, ë‚¨ì€ ì†ŒìŠ¤:', sourceNodesRef.current.size);
    }, []);

    // ========================================
    // Reset Recording
    // ========================================
    const resetRecording = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording') {
            mediaRecorderRef.current.stop();
        }

        // Clean up Web Audio API
        sourceNodesRef.current.forEach(source => {
            try {
                source.stop();
                source.disconnect();
            } catch {
                // ì´ë¯¸ ì •ì§€ë¨
            }
        });
        sourceNodesRef.current.clear();
        audioBuffersRef.current.clear();

        // Revoke all URLs
        segments.forEach(seg => URL.revokeObjectURL(seg.url));

        // Reset state
        setSegments([]);
        setState('idle');
        setIsPaused(false);
        setError(null);
        chunksRef.current = [];
        pendingRangeRef.current = null;

        console.log('ğŸ¤ All recordings reset');
    }, [state, segments]);

    return {
        state,
        permissionState,
        segments,
        recordedMeasures,
        isProcessing,
        isPaused,
        error,
        audioBlob,
        recordingRange,
        requestPermission,
        prepareRecording,
        startRecording,
        stopRecording,
        pauseJamming,
        resumeJamming,
        playRecordingsAtTime,
        pauseRecordings,
        resetRecording,
        hasRecordingAt,
        getOverlappingSegment
    };
}

export default useRecorder;
