'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import { getSharedAudioContext } from '@/utils/sharedAudioContext';

// ============================================
// Types
// ============================================
export type RecordingState = 'idle' | 'recording' | 'recorded';
export type PermissionState = 'prompt' | 'granted' | 'denied';

export interface RecordingSegment {
    id: string;
    blob: Blob;
    url: string;
    startTime: number; // ìŒì•… íƒ€ì„ë¼ì¸ ê¸°ì¤€ ì‹œì‘ ì‹œê°„
    endTime: number;   // ìŒì•… íƒ€ì„ë¼ì¸ ê¸°ì¤€ ì¢…ë£Œ ì‹œê°„
    startMeasure: number;
    endMeasure: number;
    // prerollDuration ì œê±° - ë§ˆì»¤ ê¸°ë°˜ ì¶”ì¶œë¡œ ë” ì´ìƒ í•„ìš” ì—†ìŒ
}

export interface UseRecorderOptions {
    onError?: (error: string) => void;
    onStateChange?: (state: RecordingState) => void;
}

export interface UseRecorderReturn {
    state: RecordingState;
    permissionState: PermissionState;
    segments: RecordingSegment[];
    recordedMeasures: number[];
    isProcessing: boolean;
    isPaused: boolean;
    error: string | null;
    // For save - combined blob of all segments
    audioBlob: Blob | null;
    recordingRange: { startTime: number; endTime: number; startMeasure: number; endMeasure: number } | null;
    requestPermission: () => Promise<boolean>;
    // prepareRecording ì œê±° - ë§ˆì»¤ ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ ë¶ˆí•„ìš”
    startRecording: (startTime: number, startMeasure: number) => Promise<boolean>;
    markActualStart: () => void; // ì¹´ìš´íŠ¸ë‹¤ìš´ ì™„ë£Œ ì‹œ ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆì»¤ ì°ê¸°
    stopRecording: (endTime: number, endMeasure: number) => Promise<void>;
    pauseJamming: () => void;
    resumeJamming: () => void;
    playRecordingsAtTime: (fromTime: number) => Promise<void>;
    pauseRecordings: () => void;
    resetRecording: () => void;
    hasRecordingAt: (time: number) => boolean;
    getOverlappingSegment: (startMeasure: number, endMeasure: number) => RecordingSegment | null;
}

// ============================================
// Helper: Get supported MIME type
// ============================================
function getSupportedMimeType(): string {
    const types = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg'
    ];

    for (const type of types) {
        if (MediaRecorder.isTypeSupported(type)) {
            return type;
        }
    }

    return 'audio/webm';
}

// ============================================
// Helper: Add silence padding to audio
// ============================================
async function addSilencePadding(
    audioBlob: Blob,
    silenceDuration: number
): Promise<Blob> {
    const audioContext = getSharedAudioContext();

    try {
        const arrayBuffer = await audioBlob.arrayBuffer();
        const recordedBuffer = await audioContext.decodeAudioData(arrayBuffer);

        const sampleRate = recordedBuffer.sampleRate;
        const silenceSamples = Math.floor(silenceDuration * sampleRate);
        const totalSamples = silenceSamples + recordedBuffer.length;
        const numberOfChannels = recordedBuffer.numberOfChannels;

        const offlineContext = new OfflineAudioContext(
            numberOfChannels,
            totalSamples,
            sampleRate
        );

        const combinedBuffer = offlineContext.createBuffer(
            numberOfChannels,
            totalSamples,
            sampleRate
        );

        for (let channel = 0; channel < numberOfChannels; channel++) {
            const combinedData = combinedBuffer.getChannelData(channel);
            const recordedData = recordedBuffer.getChannelData(channel);

            for (let i = 0; i < recordedBuffer.length; i++) {
                combinedData[silenceSamples + i] = recordedData[i];
            }
        }

        const source = offlineContext.createBufferSource();
        source.buffer = combinedBuffer;
        source.connect(offlineContext.destination);
        source.start();

        const renderedBuffer = await offlineContext.startRendering();
        const wavBlob = audioBufferToWavBlob(renderedBuffer);

        return wavBlob;
    } finally {
        // ê³µìœ  AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ
    }
}

// ============================================
// Helper: Convert AudioBuffer to WAV Blob
// ============================================
function audioBufferToWavBlob(buffer: AudioBuffer): Blob {
    const numChannels = buffer.numberOfChannels;
    const sampleRate = buffer.sampleRate;
    const format = 1;
    const bitDepth = 16;

    const bytesPerSample = bitDepth / 8;
    const blockAlign = numChannels * bytesPerSample;

    const dataLength = buffer.length * blockAlign;
    const bufferLength = 44 + dataLength;

    const arrayBuffer = new ArrayBuffer(bufferLength);
    const view = new DataView(arrayBuffer);

    const writeString = (offset: number, str: string) => {
        for (let i = 0; i < str.length; i++) {
            view.setUint8(offset + i, str.charCodeAt(i));
        }
    };

    writeString(0, 'RIFF');
    view.setUint32(4, bufferLength - 8, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, format, true);
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * blockAlign, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitDepth, true);
    writeString(36, 'data');
    view.setUint32(40, dataLength, true);

    let offset = 44;
    const channelData: Float32Array[] = [];
    for (let i = 0; i < numChannels; i++) {
        channelData.push(buffer.getChannelData(i));
    }

    for (let i = 0; i < buffer.length; i++) {
        for (let channel = 0; channel < numChannels; channel++) {
            const sample = Math.max(-1, Math.min(1, channelData[channel][i]));
            const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            view.setInt16(offset, intSample, true);
            offset += 2;
        }
    }

    return new Blob([arrayBuffer], { type: 'audio/wav' });
}

// ============================================
// Main Hook
// ============================================
export function useRecorder(options: UseRecorderOptions = {}): UseRecorderReturn {
    const { onError, onStateChange } = options;

    // State
    const [state, setState] = useState<RecordingState>('idle');
    const [permissionState, setPermissionState] = useState<PermissionState>('prompt');
    const [segments, setSegments] = useState<RecordingSegment[]>([]);
    const [isProcessing, setIsProcessing] = useState(false);
    const [isPaused, setIsPaused] = useState(false);
    const [error, setError] = useState<string | null>(null);

    // Refs
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const chunksRef = useRef<Blob[]>([]);
    const pendingRangeRef = useRef<{ startTime: number; startMeasure: number } | null>(null);
    // ë§ˆì»¤ ê¸°ë°˜ ë…¹ìŒ
    const recordingBlobStartRef = useRef<number>(0); // blob 0ì´ˆ ì‹œì  (performance.now, MediaRecorder.start() í˜¸ì¶œ ì‹œì )
    const actualStartMarkerRef = useRef<number>(0); // ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆì»¤ (blob ê¸°ì¤€ ìƒëŒ€ ì‹œê°„, ì´ˆ)
    const recordingStopTimeRef = useRef<number>(0); // ë…¹ìŒ ì¢…ë£Œ ì‹œì  (performance.now)

    // Web Audio API ê¸°ë°˜ ì¬ìƒ (ì •í™•í•œ íƒ€ì´ë° ë™ê¸°í™”)
    const audioContextRef = useRef<AudioContext | null>(null);
    const audioBuffersRef = useRef<Map<string, AudioBuffer>>(new Map());
    const sourceNodesRef = useRef<Map<string, AudioBufferSourceNode>>(new Map());
    const gainNodeRef = useRef<GainNode | null>(null);

    // Computed: recorded measures from all segments
    const recordedMeasures = segments.flatMap(seg => {
        const measures: number[] = [];
        for (let m = seg.startMeasure; m <= seg.endMeasure; m++) {
            measures.push(m);
        }
        return measures;
    }).filter((m, i, arr) => arr.indexOf(m) === i); // unique

    // Computed: combined blob for save (first segment for now, can be extended)
    const audioBlob = segments.length > 0 ? segments[0].blob : null;
    const recordingRange = segments.length > 0 ? {
        startTime: Math.min(...segments.map(s => s.startTime)),
        endTime: Math.max(...segments.map(s => s.endTime)),
        startMeasure: Math.min(...segments.map(s => s.startMeasure)),
        endMeasure: Math.max(...segments.map(s => s.endMeasure))
    } : null;

    // State change callback
    useEffect(() => {
        onStateChange?.(state);
    }, [state, onStateChange]);

    // Cleanup on unmount
    useEffect(() => {
        return () => {
            segments.forEach(seg => URL.revokeObjectURL(seg.url));
            // Web Audio API ì •ë¦¬
            sourceNodesRef.current.forEach(source => {
                try {
                    source.stop();
                    source.disconnect();
                } catch {
                    // ì´ë¯¸ ì •ì§€ë¨
                }
            });
            sourceNodesRef.current.clear();
            audioBuffersRef.current.clear();
            // ê³µìœ  AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ - refë§Œ ì´ˆê¸°í™”
            audioContextRef.current = null;
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
            }
        };
    }, []); // eslint-disable-line react-hooks-exhaustive-deps

    // ========================================
    // Check/Request Permission
    // ========================================
    const requestPermission = useCallback(async (): Promise<boolean> => {
        try {
            if (typeof MediaRecorder === 'undefined') {
                const msg = 'ì´ ë¸Œë¼ìš°ì €ëŠ” ë…¹ìŒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤';
                setError(msg);
                onError?.(msg);
                return false;
            }

            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            streamRef.current = stream;
            setPermissionState('granted');
            setError(null);
            return true;
        } catch (err) {
            console.error('Permission denied:', err);
            setPermissionState('denied');
            const msg = 'ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ í—ˆìš©í•´ì£¼ì„¸ìš”';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [onError]);

    // ========================================
    // Get overlapping segment
    // ========================================
    const getOverlappingSegment = useCallback((startMeasure: number, endMeasure: number): RecordingSegment | null => {
        return segments.find(seg => {
            // Check if ranges overlap
            return !(endMeasure < seg.startMeasure || startMeasure > seg.endMeasure);
        }) || null;
    }, [segments]);

    // ========================================
    // Has recording at time
    // ========================================
    const hasRecordingAt = useCallback((time: number): boolean => {
        return segments.some(seg => time >= seg.startTime && time <= seg.endTime);
    }, [segments]);

    // prepareRecording ì œê±° - ë§ˆì»¤ ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ ë¶ˆí•„ìš”

    // ========================================
    // Start Recording (ë§ˆì»¤ ê¸°ë°˜ ë…¹ìŒ ì‹œì‘)
    // Rí‚¤ ì²« ë²ˆì§¸ ëˆ„ë¥¼ ë•Œ í˜¸ì¶œ - MediaRecorder ì‹œì‘ + blob ì‹œì‘ì  ê¸°ë¡
    // ========================================
    const startRecording = useCallback(async (
        startTime: number,      // ìŒì•… íƒ€ì„ë¼ì¸ ê¸°ì¤€ ë…¹ìŒ ì‹œì‘ ì‹œê°„ (ì¹´ìš´íŠ¸ë‹¤ìš´ ëë‚˜ëŠ” ì‹œì )
        startMeasure: number    // ë…¹ìŒ ì‹œì‘ ë§ˆë””
    ): Promise<boolean> => {
        if (permissionState !== 'granted') {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        if (!streamRef.current) {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        try {
            chunksRef.current = [];
            const mimeType = getSupportedMimeType();

            const mediaRecorder = new MediaRecorder(streamRef.current!, { mimeType });
            mediaRecorderRef.current = mediaRecorder;

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunksRef.current.push(event.data);
                }
            };

            // blob ì‹œì‘ ì‹œì  ê¸°ë¡ (MediaRecorder.start() ì‹œì )
            const blobStartTime = performance.now();
            recordingBlobStartRef.current = blobStartTime;
            actualStartMarkerRef.current = 0; // ì•„ì§ ì‹¤ì œ ì‹œì‘ ì•ˆ í•¨, markActualStart()ì—ì„œ ì„¤ì •
            pendingRangeRef.current = { startTime, startMeasure };

            mediaRecorder.start(100); // 100ms timeslice
            setState('recording');
            setError(null);

            console.log('ğŸ¤ [Marker Recording] MediaRecorder.start():', {
                blobStartTime: blobStartTime.toFixed(0) + 'ms',
                targetMeasure: startMeasure,
                targetTime: startTime.toFixed(3) + 's',
                note: 'ì¹´ìš´íŠ¸ë‹¤ìš´ ëë‚˜ë©´ markActualStart() í˜¸ì¶œ í•„ìš”'
            });
            return true;
        } catch (err) {
            console.error('Start recording error:', err);
            const msg = 'ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [permissionState, requestPermission, onError]);

    // ========================================
    // Mark Actual Start (ì¹´ìš´íŠ¸ë‹¤ìš´ ì™„ë£Œ ì‹œ ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆì»¤ ì°ê¸°)
    // ========================================
    const markActualStart = useCallback(() => {
        if (state !== 'recording') {
            console.warn('ğŸ¤ [markActualStart] ë…¹ìŒ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤');
            return;
        }

        const now = performance.now();
        const markerTime = (now - recordingBlobStartRef.current) / 1000; // blob ê¸°ì¤€ ìƒëŒ€ ì‹œê°„ (ì´ˆ)
        actualStartMarkerRef.current = markerTime;

        console.log('ğŸ¤ [Marker] ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆì»¤ ì„¤ì •:', {
            blobStartTime: recordingBlobStartRef.current.toFixed(0) + 'ms',
            currentTime: now.toFixed(0) + 'ms',
            markerTime: markerTime.toFixed(3) + 's (blob ê¸°ì¤€)',
            note: 'ì´ ì‹œì ë¶€í„°ê°€ ì‹¤ì œ ë…¹ìŒ êµ¬ê°„'
        });
    }, [state]);

    // ========================================
    // Stop Recording
    // ========================================
    const stopRecording = useCallback(async (
        endTime: number,
        endMeasure: number
    ): Promise<void> => {
        if (!mediaRecorderRef.current || state !== 'recording') {
            return;
        }

        return new Promise((resolve) => {
            const mediaRecorder = mediaRecorderRef.current!;

            mediaRecorder.onstop = async () => {
                console.log('ğŸ¤ [Marker Recording] MediaRecorder stopped, processing...');
                setIsProcessing(true);

                try {
                    // ì¢…ë£Œ ë§ˆì»¤ ê¸°ë¡ (blob ê¸°ì¤€ ìƒëŒ€ ì‹œê°„)
                    const now = performance.now();
                    const endMarker = (now - recordingBlobStartRef.current) / 1000; // ì´ˆ

                    const rawBlob = new Blob(chunksRef.current, {
                        type: mediaRecorder.mimeType
                    });

                    const startTime = pendingRangeRef.current?.startTime || 0;
                    const startMeasure = pendingRangeRef.current?.startMeasure || 1;
                    const startMarker = actualStartMarkerRef.current; // ì¹´ìš´íŠ¸ë‹¤ìš´ ëë‚œ ì‹œì  (blob ê¸°ì¤€)

                    console.log('ğŸ¤ [Marker] ë§ˆì»¤ ì •ë³´:', {
                        blobStartTime: recordingBlobStartRef.current.toFixed(0) + 'ms',
                        startMarker: startMarker.toFixed(3) + 's (blob ê¸°ì¤€)',
                        endMarker: endMarker.toFixed(3) + 's (blob ê¸°ì¤€)',
                        extractDuration: (endMarker - startMarker).toFixed(3) + 's',
                        musicTimelineStart: startTime.toFixed(3) + 's',
                        musicTimelineEnd: endTime.toFixed(3) + 's',
                        measures: `${startMeasure}-${endMeasure}`
                    });

                    const segmentId = `seg-${Date.now()}`;

                    // ========================================
                    // ë§ˆì»¤ ê¸°ë°˜ êµ¬ê°„ ì¶”ì¶œ (ë‹¨ìˆœí•˜ê³  ì •í™•í•¨)
                    // ========================================
                    // AudioContext ìƒì„± (ì—†ìœ¼ë©´)
                    if (!audioContextRef.current) {
                        audioContextRef.current = getSharedAudioContext();
                        gainNodeRef.current = audioContextRef.current.createGain();
                        gainNodeRef.current.connect(audioContextRef.current.destination);
                    }

                    const arrayBuffer = await rawBlob.arrayBuffer();
                    const audioBuffer = await audioContextRef.current.decodeAudioData(arrayBuffer);

                    const blobDuration = audioBuffer.duration;
                    console.log('ğŸ¤ [Extract] Blob ì „ì²´ ê¸¸ì´:', blobDuration.toFixed(3) + 's');

                    // startMarker ~ endMarker êµ¬ê°„ë§Œ ì¶”ì¶œ
                    const sampleRate = audioBuffer.sampleRate;
                    const startSample = Math.floor(startMarker * sampleRate);
                    const endSample = Math.floor(endMarker * sampleRate);
                    const extractLength = endSample - startSample;

                    if (extractLength <= 0) {
                        throw new Error('ì¶”ì¶œí•  êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤. startMarker >= endMarker');
                    }

                    console.log('ğŸ¤ [Extract] êµ¬ê°„ ì¶”ì¶œ ì¤€ë¹„:', {
                        startSample,
                        endSample,
                        extractLength,
                        extractDuration: (extractLength / sampleRate).toFixed(3) + 's'
                    });

                    // OfflineAudioContextë¡œ êµ¬ê°„ ì¶”ì¶œ
                    const offlineCtx = new OfflineAudioContext(
                        audioBuffer.numberOfChannels,
                        extractLength,
                        sampleRate
                    );

                    const source = offlineCtx.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(offlineCtx.destination);
                    source.start(0, startMarker, endMarker - startMarker); // startMarkerë¶€í„° (endMarker-startMarker) ê¸¸ì´ë§Œí¼

                    const extractedBuffer = await offlineCtx.startRendering();

                    console.log('ğŸ¤ [Extract] ì¶”ì¶œ ì™„ë£Œ:', {
                        ì›ë³¸ê¸¸ì´: blobDuration.toFixed(3) + 's',
                        ì¶”ì¶œê¸¸ì´: extractedBuffer.duration.toFixed(3) + 's',
                        startMarker: startMarker.toFixed(3) + 's',
                        endMarker: endMarker.toFixed(3) + 's'
                    });

                    // WAVë¡œ ë³€í™˜
                    const wavBlob = audioBufferToWavBlob(extractedBuffer);
                    const url = URL.createObjectURL(wavBlob);

                    // AudioBuffer ì €ì¥ (ì¬ìƒìš©)
                    audioBuffersRef.current.set(segmentId, extractedBuffer);

                    // Segment ìƒì„±
                    const newSegment: RecordingSegment = {
                        id: segmentId,
                        blob: wavBlob,
                        url,
                        startTime,
                        endTime,
                        startMeasure,
                        endMeasure
                    };

                    // ê¸°ì¡´ ê²¹ì¹˜ëŠ” segment ì²˜ë¦¬ + ìƒˆ segment ì¶”ê°€
                    setSegments(prev => {
                        const result: RecordingSegment[] = [];

                        for (const seg of prev) {
                            const overlaps = !(endMeasure < seg.startMeasure || startMeasure > seg.endMeasure);

                            if (!overlaps) {
                                result.push(seg);
                            } else {
                                // ê²¹ì¹˜ëŠ” segment ì •ë¦¬
                                console.log('ğŸ¤ Removing overlapped segment:', seg.id, `(${seg.startMeasure}-${seg.endMeasure})`);
                                URL.revokeObjectURL(seg.url);
                                const sourceNode = sourceNodesRef.current.get(seg.id);
                                if (sourceNode) {
                                    try {
                                        sourceNode.stop();
                                        sourceNode.disconnect();
                                    } catch { /* ì´ë¯¸ ì •ì§€ë¨ */ }
                                    sourceNodesRef.current.delete(seg.id);
                                }
                                audioBuffersRef.current.delete(seg.id);
                            }
                        }

                        return [...result, newSegment];
                    });

                    setState('recorded');

                    console.log('ğŸ¤ [Marker Recording] ì™„ë£Œ:', {
                        segmentId,
                        measures: `${startMeasure}-${endMeasure}`,
                        duration: extractedBuffer.duration.toFixed(3) + 's',
                        blobSize: wavBlob.size,
                        note: 'ë§ˆì»¤ ê¸°ë°˜ ì¶”ì¶œ ì™„ë£Œ - preroll ì—†ìŒ'
                    });
                } catch (err) {
                    console.error('Processing error:', err);
                    setError('ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                    onError?.('ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                } finally {
                    setIsProcessing(false);
                    pendingRangeRef.current = null;
                    resolve();
                }
            };

            mediaRecorder.stop();
        });
    }, [state, onError]);

    // ========================================
    // Pause Jamming
    // ========================================
    const pauseJamming = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording' && !isPaused) {
            mediaRecorderRef.current.pause();
            setIsPaused(true);
            console.log('ğŸ¤ Recording paused');
        }
    }, [state, isPaused]);

    // ========================================
    // Resume Jamming
    // ========================================
    const resumeJamming = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording' && isPaused) {
            mediaRecorderRef.current.resume();
            setIsPaused(false);
            console.log('ğŸ¤ Recording resumed');
        }
    }, [state, isPaused]);

    // ========================================
    // Play Recordings At Time (Web Audio API ê¸°ë°˜ - ì •í™•í•œ íƒ€ì´ë°)
    // ========================================
    const playRecordingsAtTime = useCallback(async (fromTime: number) => {
        // Find segments that include this time
        const activeSegments = segments.filter(seg =>
            fromTime >= seg.startTime && fromTime <= seg.endTime
        );

        if (activeSegments.length === 0) {
            return;
        }

        // AudioContext ìƒì„± (ì—†ìœ¼ë©´)
        if (!audioContextRef.current) {
            audioContextRef.current = getSharedAudioContext();
            gainNodeRef.current = audioContextRef.current.createGain();
            gainNodeRef.current.connect(audioContextRef.current.destination);
        }

        const context = audioContextRef.current;

        // suspended ìƒíƒœë©´ resume (ë°˜ë“œì‹œ await!)
        if (context.state === 'suspended') {
            console.log('ğŸ¤ [Web Audio] Resuming suspended AudioContext');
            await context.resume();
        }

        console.log('ğŸ¤ [Web Audio] AudioContext state:', context.state, 'gainNode:', gainNodeRef.current?.gain.value);

        // Play each active segment using Web Audio API
        activeSegments.forEach(seg => {
            const audioBuffer = audioBuffersRef.current.get(seg.id);
            if (!audioBuffer) {
                console.warn('ğŸ¤ AudioBuffer not ready for segment:', seg.id);
                return;
            }

            // ê¸°ì¡´ source node ì •ë¦¬
            const existingSource = sourceNodesRef.current.get(seg.id);
            if (existingSource) {
                try {
                    existingSource.stop();
                    existingSource.disconnect();
                } catch {
                    // ì´ë¯¸ ì •ì§€ë¨
                }
            }

            // ë¸”ë¡­ ë‚´ ì˜¤í”„ì…‹ ê³„ì‚° (ë§ˆì»¤ ê¸°ë°˜ - preroll ì—†ìŒ)
            // fromTime - seg.startTime: ê³¡ ì‹œê°„ ë‚´ ì˜¤í”„ì…‹
            const offset = Math.max(0, fromTime - seg.startTime);

            // ìƒˆ source node ìƒì„±
            const source = context.createBufferSource();
            source.buffer = audioBuffer;
            if (gainNodeRef.current) {
                source.connect(gainNodeRef.current);
            } else {
                source.connect(context.destination);
            }

            // ì¦‰ì‹œ ì¬ìƒ (Web Audio APIëŠ” ì •í™•í•œ íƒ€ì´ë° ë³´ì¥)
            source.start(0, offset);
            sourceNodesRef.current.set(seg.id, source);

            console.log('ğŸ¤ [Web Audio] ì¬ìƒ ì‹œì‘:', {
                segId: seg.id,
                fromTime,
                segStartTime: seg.startTime,
                offset: offset.toFixed(3),
                bufferDuration: audioBuffer.duration.toFixed(2)
            });

            // ì¬ìƒ ì™„ë£Œ ì‹œ ì •ë¦¬
            source.onended = () => {
                sourceNodesRef.current.delete(seg.id);
            };
        });
    }, [segments]);

    // ========================================
    // Pause Recordings (Web Audio API ê¸°ë°˜)
    // ========================================
    const pauseRecordings = useCallback(() => {
        const count = sourceNodesRef.current.size;
        console.log('ğŸ¤ [pauseRecordings] í˜¸ì¶œë¨, í™œì„± ì†ŒìŠ¤:', count);

        sourceNodesRef.current.forEach((source, id) => {
            try {
                source.stop();
                source.disconnect();
                console.log('ğŸ¤ [pauseRecordings] ì†ŒìŠ¤ ì •ì§€:', id);
            } catch (e) {
                console.log('ğŸ¤ [pauseRecordings] ì†ŒìŠ¤ ì´ë¯¸ ì •ì§€ë¨:', id);
            }
        });
        sourceNodesRef.current.clear();
        console.log('ğŸ¤ [pauseRecordings] ì™„ë£Œ, ë‚¨ì€ ì†ŒìŠ¤:', sourceNodesRef.current.size);
    }, []);

    // ========================================
    // Reset Recording
    // ========================================
    const resetRecording = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording') {
            mediaRecorderRef.current.stop();
        }

        // Clean up Web Audio API
        sourceNodesRef.current.forEach(source => {
            try {
                source.stop();
                source.disconnect();
            } catch {
                // ì´ë¯¸ ì •ì§€ë¨
            }
        });
        sourceNodesRef.current.clear();
        audioBuffersRef.current.clear();

        // Revoke all URLs
        segments.forEach(seg => URL.revokeObjectURL(seg.url));

        // Reset state
        setSegments([]);
        setState('idle');
        setIsPaused(false);
        setError(null);
        chunksRef.current = [];
        pendingRangeRef.current = null;

        console.log('ğŸ¤ All recordings reset');
    }, [state, segments]);

    return {
        state,
        permissionState,
        segments,
        recordedMeasures,
        isProcessing,
        isPaused,
        error,
        audioBlob,
        recordingRange,
        requestPermission,
        startRecording,
        markActualStart,
        stopRecording,
        pauseJamming,
        resumeJamming,
        playRecordingsAtTime,
        pauseRecordings,
        resetRecording,
        hasRecordingAt,
        getOverlappingSegment
    };
}

export default useRecorder;
