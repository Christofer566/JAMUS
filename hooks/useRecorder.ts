'use client';

import { useState, useRef, useCallback, useEffect } from 'react';

// ============================================
// Types
// ============================================
export type RecordingState = 'idle' | 'recording' | 'recorded';
export type PermissionState = 'prompt' | 'granted' | 'denied';

export interface RecordingSegment {
    id: string;
    blob: Blob;
    url: string;
    startTime: number;
    endTime: number;
    startMeasure: number;
    endMeasure: number;
}

export interface UseRecorderOptions {
    onError?: (error: string) => void;
    onStateChange?: (state: RecordingState) => void;
}

export interface UseRecorderReturn {
    state: RecordingState;
    permissionState: PermissionState;
    segments: RecordingSegment[];
    recordedMeasures: number[];
    isProcessing: boolean;
    isPaused: boolean;
    error: string | null;
    // For save - combined blob of all segments
    audioBlob: Blob | null;
    recordingRange: { startTime: number; endTime: number; startMeasure: number; endMeasure: number } | null;
    requestPermission: () => Promise<boolean>;
    startRecording: (startTime: number, startMeasure: number) => Promise<boolean>;
    stopRecording: (endTime: number, endMeasure: number) => Promise<void>;
    pauseJamming: () => void;
    resumeJamming: () => void;
    playRecordingsAtTime: (fromTime: number) => Promise<void>;
    pauseRecordings: () => void;
    resetRecording: () => void;
    hasRecordingAt: (time: number) => boolean;
    getOverlappingSegment: (startMeasure: number, endMeasure: number) => RecordingSegment | null;
}

// ============================================
// Helper: Get supported MIME type
// ============================================
function getSupportedMimeType(): string {
    const types = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg'
    ];

    for (const type of types) {
        if (MediaRecorder.isTypeSupported(type)) {
            return type;
        }
    }

    return 'audio/webm';
}

// ============================================
// Helper: Add silence padding to audio
// ============================================
async function addSilencePadding(
    audioBlob: Blob,
    silenceDuration: number
): Promise<Blob> {
    const audioContext = new AudioContext();

    try {
        const arrayBuffer = await audioBlob.arrayBuffer();
        const recordedBuffer = await audioContext.decodeAudioData(arrayBuffer);

        const sampleRate = recordedBuffer.sampleRate;
        const silenceSamples = Math.floor(silenceDuration * sampleRate);
        const totalSamples = silenceSamples + recordedBuffer.length;
        const numberOfChannels = recordedBuffer.numberOfChannels;

        const offlineContext = new OfflineAudioContext(
            numberOfChannels,
            totalSamples,
            sampleRate
        );

        const combinedBuffer = offlineContext.createBuffer(
            numberOfChannels,
            totalSamples,
            sampleRate
        );

        for (let channel = 0; channel < numberOfChannels; channel++) {
            const combinedData = combinedBuffer.getChannelData(channel);
            const recordedData = recordedBuffer.getChannelData(channel);

            for (let i = 0; i < recordedBuffer.length; i++) {
                combinedData[silenceSamples + i] = recordedData[i];
            }
        }

        const source = offlineContext.createBufferSource();
        source.buffer = combinedBuffer;
        source.connect(offlineContext.destination);
        source.start();

        const renderedBuffer = await offlineContext.startRendering();
        const wavBlob = audioBufferToWavBlob(renderedBuffer);

        return wavBlob;
    } finally {
        await audioContext.close();
    }
}

// ============================================
// Helper: Convert AudioBuffer to WAV Blob
// ============================================
function audioBufferToWavBlob(buffer: AudioBuffer): Blob {
    const numChannels = buffer.numberOfChannels;
    const sampleRate = buffer.sampleRate;
    const format = 1;
    const bitDepth = 16;

    const bytesPerSample = bitDepth / 8;
    const blockAlign = numChannels * bytesPerSample;

    const dataLength = buffer.length * blockAlign;
    const bufferLength = 44 + dataLength;

    const arrayBuffer = new ArrayBuffer(bufferLength);
    const view = new DataView(arrayBuffer);

    const writeString = (offset: number, str: string) => {
        for (let i = 0; i < str.length; i++) {
            view.setUint8(offset + i, str.charCodeAt(i));
        }
    };

    writeString(0, 'RIFF');
    view.setUint32(4, bufferLength - 8, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, format, true);
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * blockAlign, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitDepth, true);
    writeString(36, 'data');
    view.setUint32(40, dataLength, true);

    let offset = 44;
    const channelData: Float32Array[] = [];
    for (let i = 0; i < numChannels; i++) {
        channelData.push(buffer.getChannelData(i));
    }

    for (let i = 0; i < buffer.length; i++) {
        for (let channel = 0; channel < numChannels; channel++) {
            const sample = Math.max(-1, Math.min(1, channelData[channel][i]));
            const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            view.setInt16(offset, intSample, true);
            offset += 2;
        }
    }

    return new Blob([arrayBuffer], { type: 'audio/wav' });
}

// ============================================
// Main Hook
// ============================================
export function useRecorder(options: UseRecorderOptions = {}): UseRecorderReturn {
    const { onError, onStateChange } = options;

    // State
    const [state, setState] = useState<RecordingState>('idle');
    const [permissionState, setPermissionState] = useState<PermissionState>('prompt');
    const [segments, setSegments] = useState<RecordingSegment[]>([]);
    const [isProcessing, setIsProcessing] = useState(false);
    const [isPaused, setIsPaused] = useState(false);
    const [error, setError] = useState<string | null>(null);

    // Refs
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const chunksRef = useRef<Blob[]>([]);
    const pendingRangeRef = useRef<{ startTime: number; startMeasure: number } | null>(null);
    const recordingActualStartRef = useRef<number>(0); // ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ì‹œì  (performance.now)

    // Web Audio API ê¸°ë°˜ ì¬ìƒ (ì •í™•í•œ íƒ€ì´ë° ë™ê¸°í™”)
    const audioContextRef = useRef<AudioContext | null>(null);
    const audioBuffersRef = useRef<Map<string, AudioBuffer>>(new Map());
    const sourceNodesRef = useRef<Map<string, AudioBufferSourceNode>>(new Map());
    const gainNodeRef = useRef<GainNode | null>(null);

    // Computed: recorded measures from all segments
    const recordedMeasures = segments.flatMap(seg => {
        const measures: number[] = [];
        for (let m = seg.startMeasure; m <= seg.endMeasure; m++) {
            measures.push(m);
        }
        return measures;
    }).filter((m, i, arr) => arr.indexOf(m) === i); // unique

    // Computed: combined blob for save (first segment for now, can be extended)
    const audioBlob = segments.length > 0 ? segments[0].blob : null;
    const recordingRange = segments.length > 0 ? {
        startTime: Math.min(...segments.map(s => s.startTime)),
        endTime: Math.max(...segments.map(s => s.endTime)),
        startMeasure: Math.min(...segments.map(s => s.startMeasure)),
        endMeasure: Math.max(...segments.map(s => s.endMeasure))
    } : null;

    // State change callback
    useEffect(() => {
        onStateChange?.(state);
    }, [state, onStateChange]);

    // Cleanup on unmount
    useEffect(() => {
        return () => {
            segments.forEach(seg => URL.revokeObjectURL(seg.url));
            // Web Audio API ì •ë¦¬
            sourceNodesRef.current.forEach(source => {
                try {
                    source.stop();
                    source.disconnect();
                } catch {
                    // ì´ë¯¸ ì •ì§€ë¨
                }
            });
            sourceNodesRef.current.clear();
            audioBuffersRef.current.clear();
            if (audioContextRef.current) {
                audioContextRef.current.close();
                audioContextRef.current = null;
            }
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
            }
        };
    }, []); // eslint-disable-line react-hooks-exhaustive-deps

    // ========================================
    // Check/Request Permission
    // ========================================
    const requestPermission = useCallback(async (): Promise<boolean> => {
        try {
            if (typeof MediaRecorder === 'undefined') {
                const msg = 'ì´ ë¸Œë¼ìš°ì €ëŠ” ë…¹ìŒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤';
                setError(msg);
                onError?.(msg);
                return false;
            }

            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            streamRef.current = stream;
            setPermissionState('granted');
            setError(null);
            return true;
        } catch (err) {
            console.error('Permission denied:', err);
            setPermissionState('denied');
            const msg = 'ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ í—ˆìš©í•´ì£¼ì„¸ìš”';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [onError]);

    // ========================================
    // Get overlapping segment
    // ========================================
    const getOverlappingSegment = useCallback((startMeasure: number, endMeasure: number): RecordingSegment | null => {
        return segments.find(seg => {
            // Check if ranges overlap
            return !(endMeasure < seg.startMeasure || startMeasure > seg.endMeasure);
        }) || null;
    }, [segments]);

    // ========================================
    // Has recording at time
    // ========================================
    const hasRecordingAt = useCallback((time: number): boolean => {
        return segments.some(seg => time >= seg.startTime && time <= seg.endTime);
    }, [segments]);

    // ========================================
    // Start Recording
    // ========================================
    const startRecording = useCallback(async (
        startTime: number,
        startMeasure: number
    ): Promise<boolean> => {
        if (permissionState !== 'granted') {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        if (!streamRef.current) {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        try {
            chunksRef.current = [];

            const mimeType = getSupportedMimeType();
            console.log('ğŸ¤ Recording with MIME type:', mimeType);

            const mediaRecorder = new MediaRecorder(streamRef.current!, { mimeType });
            mediaRecorderRef.current = mediaRecorder;

            pendingRangeRef.current = { startTime, startMeasure };

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunksRef.current.push(event.data);
                }
            };

            // ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ì‹œì  ê¸°ë¡ (ë™ê¸°í™” ë””ë²„ê¹…ìš©)
            const startTimestamp = performance.now();
            recordingActualStartRef.current = startTimestamp;

            mediaRecorder.start(100);
            setState('recording');
            setError(null);

            console.log('ğŸ¤ Recording started:', {
                targetTime: startTime,
                measure: startMeasure,
                actualTimestamp: startTimestamp,
                hint: 'ë™ê¸°í™” í…ŒìŠ¤íŠ¸: ë©”íŠ¸ë¡œë†ˆì— ë§ì¶° ì†ë¼‰ì„ ì¹˜ê³ , ì¬ìƒ ì‹œ ë©”íŠ¸ë¡œë†ˆê³¼ ì†ë¼‰ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸'
            });
            return true;
        } catch (err) {
            console.error('Start recording error:', err);
            const msg = 'ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [permissionState, requestPermission, onError]);

    // ========================================
    // Stop Recording
    // ========================================
    const stopRecording = useCallback(async (
        endTime: number,
        endMeasure: number
    ): Promise<void> => {
        if (!mediaRecorderRef.current || state !== 'recording') {
            return;
        }

        return new Promise((resolve) => {
            const mediaRecorder = mediaRecorderRef.current!;

            mediaRecorder.onstop = async () => {
                console.log('ğŸ¤ Recording stopped, processing...');
                setIsProcessing(true);

                try {
                    const rawBlob = new Blob(chunksRef.current, {
                        type: mediaRecorder.mimeType
                    });

                    const startTime = pendingRangeRef.current?.startTime || 0;
                    const startMeasure = pendingRangeRef.current?.startMeasure || 1;

                    // ë…¹ìŒ ì‹œì‘ ì§€ì—° ë³´ì • (MediaRecorder ì´ˆê¸°í™” + ë²„í¼ë§ ì§€ì—°)
                    // ì´ ê°’ì„ ì¡°ì ˆí•˜ì—¬ ë™ê¸°í™”ë¥¼ ë§ì¶¤:
                    // - ì¬ìƒ ì‹œ ë…¹ìŒì´ ë¹ ë¥´ê²Œ ë“¤ë¦¬ë©´: ê°’ì„ ì¤„ì„ (silence padding ì¦ê°€)
                    // - ì¬ìƒ ì‹œ ë…¹ìŒì´ ëŠ¦ê²Œ ë“¤ë¦¬ë©´: ê°’ì„ ëŠ˜ë¦¼ (silence padding ê°ì†Œ)
                    // í…ŒìŠ¤íŠ¸: ë©”íŠ¸ë¡œë†ˆì— ë§ì¶° ì†ë¼‰ ë…¹ìŒ í›„, ì¬ìƒ ì‹œ ë©”íŠ¸ë¡œë†ˆê³¼ ë¹„êµ
                    const RECORDING_LATENCY_COMPENSATION = 0.12; // ë…¹ìŒì´ ëŠ¦ê²Œ ì¬ìƒë˜ì–´ ë³´ì •
                    const adjustedStartTime = Math.max(0, startTime - RECORDING_LATENCY_COMPENSATION);

                    // ì‹¤ì œ ë…¹ìŒ ì‹œê°„ê³¼ ì˜ˆìƒ ì‹œê°„ ë¹„êµ (ë””ë²„ê¹…ìš©)
                    const recordingDuration = performance.now() - recordingActualStartRef.current;
                    const expectedDuration = (endTime - startTime) * 1000; // ms

                    console.log('ğŸ¤ Recording sync debug:', {
                        expectedDuration: `${expectedDuration.toFixed(0)}ms`,
                        actualDuration: `${recordingDuration.toFixed(0)}ms`,
                        difference: `${(recordingDuration - expectedDuration).toFixed(0)}ms`,
                        latencyCompensation: `${RECORDING_LATENCY_COMPENSATION * 1000}ms`,
                        adjustedStartTime: `${adjustedStartTime.toFixed(3)}s`,
                        note: 'ë¬´ìŒ íŒ¨ë”© ì œê±°ë¨ - ìˆœìˆ˜ ë…¹ìŒ ë°ì´í„°ë§Œ ì €ì¥'
                    });
                    // ë¬´ìŒ íŒ¨ë”© ì œê±°: ìˆœìˆ˜ ë…¹ìŒ ë°ì´í„°ë§Œ ì €ì¥
                    // ì¬ìƒ ì‹œ startTime ì˜¤í”„ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë™ê¸°í™”
                    const url = URL.createObjectURL(rawBlob);
                    const segmentId = `seg-${Date.now()}`;

                    const newSegment: RecordingSegment = {
                        id: segmentId,
                        blob: rawBlob,  // ë¬´ìŒ íŒ¨ë”© ì—†ì´ ìˆœìˆ˜ ë…¹ìŒ ë°ì´í„°
                        url,
                        startTime,
                        endTime,
                        startMeasure,
                        endMeasure
                    };

                    // Web Audio API: Blobì„ AudioBufferë¡œ ë””ì½”ë”© (ì •í™•í•œ íƒ€ì´ë° ë™ê¸°í™”)
                    const decodeAudioBuffer = async () => {
                        try {
                            // AudioContext ìƒì„± (ì—†ìœ¼ë©´)
                            if (!audioContextRef.current) {
                                const AudioContextClass = window.AudioContext || (window as typeof window & { webkitAudioContext: typeof AudioContext }).webkitAudioContext;
                                audioContextRef.current = new AudioContextClass();
                                gainNodeRef.current = audioContextRef.current.createGain();
                                gainNodeRef.current.connect(audioContextRef.current.destination);
                            }

                            const arrayBuffer = await rawBlob.arrayBuffer();
                            const audioBuffer = await audioContextRef.current.decodeAudioData(arrayBuffer);
                            audioBuffersRef.current.set(segmentId, audioBuffer);
                            console.log('ğŸ¤ AudioBuffer decoded for segment:', segmentId, 'duration:', audioBuffer.duration.toFixed(2) + 's');
                        } catch (err) {
                            console.error('ğŸ¤ Failed to decode audio buffer:', err);
                        }
                    };
                    decodeAudioBuffer();

                    // Add to segments (trimming overlapping ones instead of removing)
                    setSegments(prev => {
                        const result: RecordingSegment[] = [];

                        for (const seg of prev) {
                            const overlaps = !(endMeasure < seg.startMeasure || startMeasure > seg.endMeasure);

                            if (!overlaps) {
                                // No overlap, keep as-is
                                result.push(seg);
                            } else {
                                // Overlap detected - trim instead of delete
                                // Case 1: New recording starts after existing segment start
                                // Keep the part before the new recording
                                if (seg.startMeasure < startMeasure) {
                                    // Trim existing segment to end before new recording starts
                                    const trimmedSeg: RecordingSegment = {
                                        ...seg,
                                        endMeasure: startMeasure - 1,
                                        endTime: startTime // Use new recording's start time as end
                                    };
                                    console.log('ğŸ¤ Trimming segment', seg.id, 'from', seg.startMeasure, '-', seg.endMeasure, 'to', trimmedSeg.startMeasure, '-', trimmedSeg.endMeasure);
                                    result.push(trimmedSeg);
                                }
                                // Case 2: New recording ends before existing segment end
                                // Keep the part after the new recording (less common case)
                                else if (seg.endMeasure > endMeasure) {
                                    // Trim existing segment to start after new recording ends
                                    const trimmedSeg: RecordingSegment = {
                                        ...seg,
                                        startMeasure: endMeasure + 1,
                                        startTime: endTime // Use new recording's end time as start
                                    };
                                    console.log('ğŸ¤ Trimming segment', seg.id, 'from', seg.startMeasure, '-', seg.endMeasure, 'to', trimmedSeg.startMeasure, '-', trimmedSeg.endMeasure);
                                    result.push(trimmedSeg);
                                }
                                // Case 3: New recording completely covers existing segment
                                else {
                                    // Remove entirely
                                    console.log('ğŸ¤ Removing completely overlapped segment', seg.id);
                                    URL.revokeObjectURL(seg.url);
                                    // Web Audio API ì •ë¦¬
                                    const source = sourceNodesRef.current.get(seg.id);
                                    if (source) {
                                        try {
                                            source.stop();
                                            source.disconnect();
                                        } catch {
                                            // ì´ë¯¸ ì •ì§€ë¨
                                        }
                                        sourceNodesRef.current.delete(seg.id);
                                    }
                                    audioBuffersRef.current.delete(seg.id);
                                }
                            }
                        }

                        return [...result, newSegment];
                    });

                    setState('recorded');

                    console.log('ğŸ¤ Recording complete:', {
                        id: segmentId,
                        duration: endTime - startTime,
                        measures: `${startMeasure}-${endMeasure}`,
                        blobSize: rawBlob.size,
                        note: 'ë¬´ìŒ íŒ¨ë”© ì—†ìŒ'
                    });
                } catch (err) {
                    console.error('Processing error:', err);
                    setError('ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                    onError?.('ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                } finally {
                    setIsProcessing(false);
                    pendingRangeRef.current = null;
                    resolve();
                }
            };

            mediaRecorder.stop();
        });
    }, [state, onError]);

    // ========================================
    // Pause Jamming
    // ========================================
    const pauseJamming = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording' && !isPaused) {
            mediaRecorderRef.current.pause();
            setIsPaused(true);
            console.log('ğŸ¤ Recording paused');
        }
    }, [state, isPaused]);

    // ========================================
    // Resume Jamming
    // ========================================
    const resumeJamming = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording' && isPaused) {
            mediaRecorderRef.current.resume();
            setIsPaused(false);
            console.log('ğŸ¤ Recording resumed');
        }
    }, [state, isPaused]);

    // ========================================
    // Play Recordings At Time (Web Audio API ê¸°ë°˜ - ì •í™•í•œ íƒ€ì´ë°)
    // ========================================
    const playRecordingsAtTime = useCallback(async (fromTime: number) => {
        // Find segments that include this time
        const activeSegments = segments.filter(seg =>
            fromTime >= seg.startTime && fromTime <= seg.endTime
        );

        if (activeSegments.length === 0) {
            return;
        }

        // AudioContext ìƒì„± (ì—†ìœ¼ë©´)
        if (!audioContextRef.current) {
            const AudioContextClass = window.AudioContext || (window as typeof window & { webkitAudioContext: typeof AudioContext }).webkitAudioContext;
            audioContextRef.current = new AudioContextClass();
            gainNodeRef.current = audioContextRef.current.createGain();
            gainNodeRef.current.connect(audioContextRef.current.destination);
        }

        const context = audioContextRef.current;

        // suspended ìƒíƒœë©´ resume (ë°˜ë“œì‹œ await!)
        if (context.state === 'suspended') {
            console.log('ğŸ¤ [Web Audio] Resuming suspended AudioContext');
            await context.resume();
        }

        console.log('ğŸ¤ [Web Audio] AudioContext state:', context.state, 'gainNode:', gainNodeRef.current?.gain.value);

        // Play each active segment using Web Audio API
        activeSegments.forEach(seg => {
            const audioBuffer = audioBuffersRef.current.get(seg.id);
            if (!audioBuffer) {
                console.warn('ğŸ¤ AudioBuffer not ready for segment:', seg.id);
                return;
            }

            // ê¸°ì¡´ source node ì •ë¦¬
            const existingSource = sourceNodesRef.current.get(seg.id);
            if (existingSource) {
                try {
                    existingSource.stop();
                    existingSource.disconnect();
                } catch {
                    // ì´ë¯¸ ì •ì§€ë¨
                }
            }

            // ë¸”ë¡­ ë‚´ ì˜¤í”„ì…‹ ê³„ì‚° (ë¬´ìŒ íŒ¨ë”© ì—†ìŒ)
            // ë¸”ë¡­ ì‹œê°„ 0 = ê³¡ ì‹œê°„ startTime
            const offset = Math.max(0, fromTime - seg.startTime);

            // ìƒˆ source node ìƒì„±
            const source = context.createBufferSource();
            source.buffer = audioBuffer;
            if (gainNodeRef.current) {
                source.connect(gainNodeRef.current);
            } else {
                source.connect(context.destination);
            }

            // ì¦‰ì‹œ ì¬ìƒ (Web Audio APIëŠ” ì •í™•í•œ íƒ€ì´ë° ë³´ì¥)
            source.start(0, offset);
            sourceNodesRef.current.set(seg.id, source);

            console.log('ğŸ¤ [Web Audio] ì¬ìƒ ì‹œì‘:', {
                segId: seg.id,
                fromTime,
                segStartTime: seg.startTime,
                offset: offset.toFixed(3),
                bufferDuration: audioBuffer.duration.toFixed(2)
            });

            // ì¬ìƒ ì™„ë£Œ ì‹œ ì •ë¦¬
            source.onended = () => {
                sourceNodesRef.current.delete(seg.id);
            };
        });
    }, [segments]);

    // ========================================
    // Pause Recordings (Web Audio API ê¸°ë°˜)
    // ========================================
    const pauseRecordings = useCallback(() => {
        sourceNodesRef.current.forEach((source, id) => {
            try {
                source.stop();
                source.disconnect();
            } catch {
                // ì´ë¯¸ ì •ì§€ë¨
            }
        });
        sourceNodesRef.current.clear();
    }, []);

    // ========================================
    // Reset Recording
    // ========================================
    const resetRecording = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording') {
            mediaRecorderRef.current.stop();
        }

        // Clean up Web Audio API
        sourceNodesRef.current.forEach(source => {
            try {
                source.stop();
                source.disconnect();
            } catch {
                // ì´ë¯¸ ì •ì§€ë¨
            }
        });
        sourceNodesRef.current.clear();
        audioBuffersRef.current.clear();

        // Revoke all URLs
        segments.forEach(seg => URL.revokeObjectURL(seg.url));

        // Reset state
        setSegments([]);
        setState('idle');
        setIsPaused(false);
        setError(null);
        chunksRef.current = [];
        pendingRangeRef.current = null;

        console.log('ğŸ¤ All recordings reset');
    }, [state, segments]);

    return {
        state,
        permissionState,
        segments,
        recordedMeasures,
        isProcessing,
        isPaused,
        error,
        audioBlob,
        recordingRange,
        requestPermission,
        startRecording,
        stopRecording,
        pauseJamming,
        resumeJamming,
        playRecordingsAtTime,
        pauseRecordings,
        resetRecording,
        hasRecordingAt,
        getOverlappingSegment
    };
}

export default useRecorder;
