'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import { getSharedAudioContext } from '@/utils/sharedAudioContext';

// ============================================
// Types
// ============================================
export type RecordingState = 'idle' | 'recording' | 'recorded';
export type PermissionState = 'prompt' | 'granted' | 'denied';

export interface RecordingSegment {
    id: string;
    blob: Blob;
    url: string;
    startTime: number; // ìŒì•… íƒ€ì„ë¼ì¸ ê¸°ì¤€ ì‹œì‘ ì‹œê°„
    endTime: number;   // ìŒì•… íƒ€ì„ë¼ì¸ ê¸°ì¤€ ì¢…ë£Œ ì‹œê°„
    startMeasure: number;
    endMeasure: number;
    // prerollDuration ì œê±° - ë§ˆì»¤ ê¸°ë°˜ ì¶”ì¶œë¡œ ë” ì´ìƒ í•„ìš” ì—†ìŒ
}

export interface UseRecorderOptions {
    onError?: (error: string) => void;
    onStateChange?: (state: RecordingState) => void;
}

export interface UseRecorderReturn {
    state: RecordingState;
    permissionState: PermissionState;
    segments: RecordingSegment[];
    recordedMeasures: number[];
    isProcessing: boolean;
    isPaused: boolean;
    error: string | null;
    // For save - combined blob of all segments
    audioBlob: Blob | null;
    recordingRange: { startTime: number; endTime: number; startMeasure: number; endMeasure: number } | null;
    // Phase 52: Warm-up ìƒíƒœ
    isWarmedUp: boolean;
    measuredLatency: number; // ì¸¡ì •ëœ ê³ ì • ì§€ì—° (ms)
    requestPermission: () => Promise<boolean>;
    // Phase 52: Warm-up - í˜ì´ì§€ ì§„ì… ì‹œ ë§ˆì´í¬ í†µë¡œ ì‚¬ì „ í™œì„±í™”
    warmUp: () => Promise<boolean>;
    startRecording: (startTime: number, startMeasure: number) => Promise<boolean>;
    markActualStart: (timeDelta?: number) => void; // ì¹´ìš´íŠ¸ë‹¤ìš´ ì™„ë£Œ ì‹œ ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆì»¤ ì°ê¸° (timeDelta: í•˜ë“œì›¨ì–´ ì§€ì—° ë³´ì •ê°’)
    stopRecording: (endTime: number, endMeasure: number) => Promise<void>;
    pauseJamming: () => void;
    resumeJamming: () => void;
    playRecordingsAtTime: (fromTime: number) => Promise<void>;
    pauseRecordings: () => void;
    resetRecording: () => void;
    hasRecordingAt: (time: number) => boolean;
    getOverlappingSegment: (startMeasure: number, endMeasure: number) => RecordingSegment | null;
}

// ============================================
// Helper: Get supported MIME type
// ============================================
function getSupportedMimeType(): string {
    const types = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/mp4',
        'audio/ogg;codecs=opus',
        'audio/ogg'
    ];

    for (const type of types) {
        if (MediaRecorder.isTypeSupported(type)) {
            return type;
        }
    }

    return 'audio/webm';
}

// ============================================
// Helper: Add silence padding to audio
// ============================================
async function addSilencePadding(
    audioBlob: Blob,
    silenceDuration: number
): Promise<Blob> {
    const audioContext = getSharedAudioContext();

    try {
        const arrayBuffer = await audioBlob.arrayBuffer();
        const recordedBuffer = await audioContext.decodeAudioData(arrayBuffer);

        const sampleRate = recordedBuffer.sampleRate;
        const silenceSamples = Math.floor(silenceDuration * sampleRate);
        const totalSamples = silenceSamples + recordedBuffer.length;
        const numberOfChannels = recordedBuffer.numberOfChannels;

        const offlineContext = new OfflineAudioContext(
            numberOfChannels,
            totalSamples,
            sampleRate
        );

        const combinedBuffer = offlineContext.createBuffer(
            numberOfChannels,
            totalSamples,
            sampleRate
        );

        for (let channel = 0; channel < numberOfChannels; channel++) {
            const combinedData = combinedBuffer.getChannelData(channel);
            const recordedData = recordedBuffer.getChannelData(channel);

            for (let i = 0; i < recordedBuffer.length; i++) {
                combinedData[silenceSamples + i] = recordedData[i];
            }
        }

        const source = offlineContext.createBufferSource();
        source.buffer = combinedBuffer;
        source.connect(offlineContext.destination);
        source.start();

        const renderedBuffer = await offlineContext.startRendering();
        const wavBlob = audioBufferToWavBlob(renderedBuffer);

        return wavBlob;
    } finally {
        // ê³µìœ  AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ
    }
}

// ============================================
// Helper: Convert AudioBuffer to WAV Blob
// ============================================
function audioBufferToWavBlob(buffer: AudioBuffer): Blob {
    const numChannels = buffer.numberOfChannels;
    const sampleRate = buffer.sampleRate;
    const format = 1;
    const bitDepth = 16;

    const bytesPerSample = bitDepth / 8;
    const blockAlign = numChannels * bytesPerSample;

    const dataLength = buffer.length * blockAlign;
    const bufferLength = 44 + dataLength;

    const arrayBuffer = new ArrayBuffer(bufferLength);
    const view = new DataView(arrayBuffer);

    const writeString = (offset: number, str: string) => {
        for (let i = 0; i < str.length; i++) {
            view.setUint8(offset + i, str.charCodeAt(i));
        }
    };

    writeString(0, 'RIFF');
    view.setUint32(4, bufferLength - 8, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, format, true);
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * blockAlign, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitDepth, true);
    writeString(36, 'data');
    view.setUint32(40, dataLength, true);

    let offset = 44;
    const channelData: Float32Array[] = [];
    for (let i = 0; i < numChannels; i++) {
        channelData.push(buffer.getChannelData(i));
    }

    for (let i = 0; i < buffer.length; i++) {
        for (let channel = 0; channel < numChannels; channel++) {
            const sample = Math.max(-1, Math.min(1, channelData[channel][i]));
            const intSample = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
            view.setInt16(offset, intSample, true);
            offset += 2;
        }
    }

    return new Blob([arrayBuffer], { type: 'audio/wav' });
}

// ============================================
// Main Hook
// ============================================
export function useRecorder(options: UseRecorderOptions = {}): UseRecorderReturn {
    const { onError, onStateChange } = options;

    // State
    const [state, setState] = useState<RecordingState>('idle');
    const [permissionState, setPermissionState] = useState<PermissionState>('prompt');
    const [segments, setSegments] = useState<RecordingSegment[]>([]);
    const [isProcessing, setIsProcessing] = useState(false);
    const [isPaused, setIsPaused] = useState(false);
    const [error, setError] = useState<string | null>(null);
    // Phase 52: Warm-up ìƒíƒœ
    const [isWarmedUp, setIsWarmedUp] = useState(false);
    const [measuredLatency, setMeasuredLatency] = useState(0); // ì¸¡ì •ëœ ê³ ì • ì§€ì—° (ms)

    // Refs
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const chunksRef = useRef<Blob[]>([]);
    const pendingRangeRef = useRef<{ startTime: number; startMeasure: number } | null>(null);
    // ë§ˆì»¤ ê¸°ë°˜ ë…¹ìŒ
    const recordingBlobStartRef = useRef<number>(0); // blob 0ì´ˆ ì‹œì  (performance.now, MediaRecorder.start() í˜¸ì¶œ ì‹œì )
    const actualStartMarkerRef = useRef<number>(0); // ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆì»¤ (blob ê¸°ì¤€ ìƒëŒ€ ì‹œê°„, ì´ˆ)
    const recordingStopTimeRef = useRef<number>(0); // ë…¹ìŒ ì¢…ë£Œ ì‹œì  (performance.now)
    // Phase 52: Calibration - ê³ ì • ì§€ì—° ì¸¡ì •
    const firstDataTimestampRef = useRef<number>(0); // ì²« ë²ˆì§¸ ondataavailable ì´ë²¤íŠ¸ ì‹œì 
    const staticLatencyRef = useRef<number>(0); // ì¸¡ì •ëœ ê³ ì • ì§€ì—° (ms)

    // Web Audio API ê¸°ë°˜ ì¬ìƒ (ì •í™•í•œ íƒ€ì´ë° ë™ê¸°í™”)
    const audioContextRef = useRef<AudioContext | null>(null);
    const audioBuffersRef = useRef<Map<string, AudioBuffer>>(new Map());
    const sourceNodesRef = useRef<Map<string, AudioBufferSourceNode>>(new Map());
    const gainNodeRef = useRef<GainNode | null>(null);

    // Computed: recorded measures from all segments
    const recordedMeasures = segments.flatMap(seg => {
        const measures: number[] = [];
        for (let m = seg.startMeasure; m <= seg.endMeasure; m++) {
            measures.push(m);
        }
        return measures;
    }).filter((m, i, arr) => arr.indexOf(m) === i); // unique

    // Computed: combined blob for save (first segment for now, can be extended)
    const audioBlob = segments.length > 0 ? segments[0].blob : null;
    const recordingRange = segments.length > 0 ? {
        startTime: Math.min(...segments.map(s => s.startTime)),
        endTime: Math.max(...segments.map(s => s.endTime)),
        startMeasure: Math.min(...segments.map(s => s.startMeasure)),
        endMeasure: Math.max(...segments.map(s => s.endMeasure))
    } : null;

    // State change callback
    useEffect(() => {
        onStateChange?.(state);
    }, [state, onStateChange]);

    // Cleanup on unmount
    useEffect(() => {
        return () => {
            segments.forEach(seg => URL.revokeObjectURL(seg.url));
            // Web Audio API ì •ë¦¬
            sourceNodesRef.current.forEach(source => {
                try {
                    source.stop();
                    source.disconnect();
                } catch {
                    // ì´ë¯¸ ì •ì§€ë¨
                }
            });
            sourceNodesRef.current.clear();
            audioBuffersRef.current.clear();
            // ê³µìœ  AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ - refë§Œ ì´ˆê¸°í™”
            audioContextRef.current = null;
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
            }
        };
    }, []); // eslint-disable-line react-hooks-exhaustive-deps

    // ========================================
    // Check/Request Permission
    // ========================================
    const requestPermission = useCallback(async (): Promise<boolean> => {
        try {
            if (typeof MediaRecorder === 'undefined') {
                const msg = 'ì´ ë¸Œë¼ìš°ì €ëŠ” ë…¹ìŒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤';
                setError(msg);
                onError?.(msg);
                return false;
            }

            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            streamRef.current = stream;
            setPermissionState('granted');
            setError(null);
            return true;
        } catch (err) {
            console.error('Permission denied:', err);
            setPermissionState('denied');
            const msg = 'ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ í—ˆìš©í•´ì£¼ì„¸ìš”';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [onError]);

    // ========================================
    // Phase 52: Warm-up - ë§ˆì´í¬ í†µë¡œ ì‚¬ì „ í™œì„±í™”
    // í˜ì´ì§€ ì§„ì… ì‹œ í˜¸ì¶œí•˜ì—¬ MediaRecorder í•˜ë“œì›¨ì–´ ì§€ì—°ì„ ë¯¸ë¦¬ í•´ì†Œ
    // ========================================
    const warmUp = useCallback(async (): Promise<boolean> => {
        console.log('ğŸ”¥ [Phase 52 Warm-up] ë§ˆì´í¬ ì›Œë°ì—… ì‹œì‘...');

        // 1. ê¶Œí•œ í™•ë³´
        if (permissionState !== 'granted') {
            const granted = await requestPermission();
            if (!granted) {
                console.warn('ğŸ”¥ [Warm-up] ë§ˆì´í¬ ê¶Œí•œ ì—†ìŒ');
                return false;
            }
        }

        if (!streamRef.current) {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        try {
            // 2. AudioContext ë¯¸ë¦¬ í™œì„±í™”
            if (!audioContextRef.current) {
                audioContextRef.current = getSharedAudioContext();
                gainNodeRef.current = audioContextRef.current.createGain();
                gainNodeRef.current.connect(audioContextRef.current.destination);
            }

            if (audioContextRef.current.state === 'suspended') {
                await audioContextRef.current.resume();
                console.log('ğŸ”¥ [Warm-up] AudioContext resumed');
            }

            // 3. MediaRecorder ì§§ê²Œ ì‹œì‘/ì •ì§€ (í•˜ë“œì›¨ì–´ ì´ˆê¸°í™”)
            const mimeType = getSupportedMimeType();
            const warmupRecorder = new MediaRecorder(streamRef.current!, { mimeType });

            return new Promise((resolve) => {
                const warmupStartTime = performance.now();
                let firstDataTime = 0;

                warmupRecorder.ondataavailable = (event) => {
                    if (firstDataTime === 0 && event.data.size > 0) {
                        firstDataTime = performance.now();
                        const latency = firstDataTime - warmupStartTime;

                        // ê³ ì • ì§€ì—° ì¸¡ì • ë° ì €ì¥
                        staticLatencyRef.current = latency;
                        setMeasuredLatency(latency);

                        console.log('ğŸ”¥ [Warm-up] ê³ ì • ì§€ì—° ì¸¡ì • ì™„ë£Œ:', {
                            startTime: warmupStartTime.toFixed(0) + 'ms',
                            firstDataTime: firstDataTime.toFixed(0) + 'ms',
                            staticLatency: latency.toFixed(1) + 'ms',
                            note: 'ì´ ê°’ë§Œí¼ ì¶”ì¶œ ì‹œ Pull-back ì ìš©ë¨'
                        });
                    }
                };

                warmupRecorder.onstop = () => {
                    // Phase 53: ì§€ì—° ì¸¡ì • ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                    // ì¼ë°˜ì ì¸ í•˜ë“œì›¨ì–´ ì§€ì—° 250ms (ë¸Œë¼ìš°ì €/ê¸°ê¸°ë§ˆë‹¤ ë‹¤ë¦„)
                    const DEFAULT_STATIC_LATENCY_MS = 250;

                    if (staticLatencyRef.current === 0) {
                        staticLatencyRef.current = DEFAULT_STATIC_LATENCY_MS;
                        setMeasuredLatency(DEFAULT_STATIC_LATENCY_MS);
                        console.log('ğŸ”¥ [Phase 53 Warm-up] ê³ ì • ì§€ì—° ì¸¡ì • ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©:', DEFAULT_STATIC_LATENCY_MS + 'ms');
                    }

                    setIsWarmedUp(true);
                    console.log('ğŸ”¥ [Phase 52 Warm-up] ì™„ë£Œ! ë§ˆì´í¬ í†µë¡œ í™œì„±í™”ë¨, ìµœì¢… ì§€ì—°:', staticLatencyRef.current.toFixed(1) + 'ms');
                    resolve(true);
                };

                warmupRecorder.start(50); // 50ms timesliceë¡œ ë¹ ë¥´ê²Œ ë°ì´í„° ìˆ˜ì§‘

                // 300ms í›„ ì •ì§€ (ì¶©ë¶„íˆ ë°ì´í„° ìˆ˜ì§‘)
                setTimeout(() => {
                    if (warmupRecorder.state === 'recording') {
                        warmupRecorder.stop();
                    }
                }, 300);
            });
        } catch (err) {
            console.error('ğŸ”¥ [Warm-up] ì‹¤íŒ¨:', err);
            return false;
        }
    }, [permissionState, requestPermission]);

    // ========================================
    // Get overlapping segment
    // ========================================
    const getOverlappingSegment = useCallback((startMeasure: number, endMeasure: number): RecordingSegment | null => {
        return segments.find(seg => {
            // Check if ranges overlap
            return !(endMeasure < seg.startMeasure || startMeasure > seg.endMeasure);
        }) || null;
    }, [segments]);

    // ========================================
    // Has recording at time
    // ========================================
    const hasRecordingAt = useCallback((time: number): boolean => {
        return segments.some(seg => time >= seg.startTime && time <= seg.endTime);
    }, [segments]);

    // prepareRecording ì œê±° - ë§ˆì»¤ ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ ë¶ˆí•„ìš”

    // ========================================
    // Start Recording (ë§ˆì»¤ ê¸°ë°˜ ë…¹ìŒ ì‹œì‘)
    // Rí‚¤ ì²« ë²ˆì§¸ ëˆ„ë¥¼ ë•Œ í˜¸ì¶œ - MediaRecorder ì‹œì‘ + blob ì‹œì‘ì  ê¸°ë¡
    // ========================================
    const startRecording = useCallback(async (
        startTime: number,      // ìŒì•… íƒ€ì„ë¼ì¸ ê¸°ì¤€ ë…¹ìŒ ì‹œì‘ ì‹œê°„ (ì¹´ìš´íŠ¸ë‹¤ìš´ ëë‚˜ëŠ” ì‹œì )
        startMeasure: number    // ë…¹ìŒ ì‹œì‘ ë§ˆë””
    ): Promise<boolean> => {
        if (permissionState !== 'granted') {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        if (!streamRef.current) {
            const granted = await requestPermission();
            if (!granted) return false;
        }

        try {
            chunksRef.current = [];
            const mimeType = getSupportedMimeType();

            const mediaRecorder = new MediaRecorder(streamRef.current!, { mimeType });
            mediaRecorderRef.current = mediaRecorder;

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunksRef.current.push(event.data);
                }
            };

            // blob ì‹œì‘ ì‹œì  ê¸°ë¡ (MediaRecorder.start() ì‹œì )
            const blobStartTime = performance.now();
            recordingBlobStartRef.current = blobStartTime;
            actualStartMarkerRef.current = 0; // ì•„ì§ ì‹¤ì œ ì‹œì‘ ì•ˆ í•¨, markActualStart()ì—ì„œ ì„¤ì •
            pendingRangeRef.current = { startTime, startMeasure };

            mediaRecorder.start(100); // 100ms timeslice
            setState('recording');
            setError(null);

            console.log('ğŸ¤ [Marker Recording] MediaRecorder.start():', {
                blobStartTime: blobStartTime.toFixed(0) + 'ms',
                targetMeasure: startMeasure,
                targetTime: startTime.toFixed(3) + 's',
                note: 'ì¹´ìš´íŠ¸ë‹¤ìš´ ëë‚˜ë©´ markActualStart() í˜¸ì¶œ í•„ìš”'
            });
            return true;
        } catch (err) {
            console.error('Start recording error:', err);
            const msg = 'ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤';
            setError(msg);
            onError?.(msg);
            return false;
        }
    }, [permissionState, requestPermission, onError]);

    // ========================================
    // Mark Actual Start (ì¹´ìš´íŠ¸ë‹¤ìš´ ì™„ë£Œ ì‹œ ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆì»¤ ì°ê¸°)
    // Phase 31: í•˜ë“œì›¨ì–´ ì§€ì—° ë³´ì • (ëª©í‘œ ì‹œê°„ vs ì‹¤ì œ ì˜¤ë””ì˜¤ ì‹œê°„ ì°¨ì´ ë°˜ì˜)
    // ========================================
    const markActualStart = useCallback((timeDelta: number = 0) => {
        // MediaRecorderì˜ ì‹¤ì œ ìƒíƒœë¡œ ì²´í¬ (React state ì—…ë°ì´íŠ¸ ì§€ì—° ë¬¸ì œ í•´ê²°)
        if (!mediaRecorderRef.current || mediaRecorderRef.current.state !== 'recording') {
            console.warn('ğŸ¤ [markActualStart] MediaRecorderê°€ ë…¹ìŒ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤:', {
                hasRecorder: !!mediaRecorderRef.current,
                recorderState: mediaRecorderRef.current?.state,
                reactState: state
            });
            return;
        }

        const now = performance.now();
        const markerTime = (now - recordingBlobStartRef.current) / 1000; // blob ê¸°ì¤€ ìƒëŒ€ ì‹œê°„ (ì´ˆ)

        // Phase 31: í•˜ë“œì›¨ì–´ ì§€ì—° ë³´ì • ì ìš©
        // timeDelta = ëª©í‘œ ì‹œê°„ - ì‹¤ì œ ì˜¤ë””ì˜¤ ì‹œê°„ (ì˜ˆ: 13.521 - 13.512 = 0.009s)
        // markerTimeì— ë”í•˜ë©´ blobì—ì„œ ì¶”ì¶œí•  ì‹œì‘ì ì´ ì•ë‹¹ê²¨ì§ (ì§€ì—° ìƒì‡„)
        const correctedMarkerTime = markerTime + timeDelta;
        actualStartMarkerRef.current = correctedMarkerTime;

        console.log('ğŸ¤ [Marker] ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë§ˆì»¤ ì„¤ì • (í•˜ë“œì›¨ì–´ ì§€ì—° ë³´ì •):', {
            blobStartTime: recordingBlobStartRef.current.toFixed(0) + 'ms',
            currentTime: now.toFixed(0) + 'ms',
            ì›ë³¸ë§ˆì»¤: markerTime.toFixed(3) + 's (blob ê¸°ì¤€)',
            ì§€ì—°ë³´ì •: timeDelta.toFixed(3) + 's',
            ìµœì¢…ë§ˆì»¤: correctedMarkerTime.toFixed(3) + 's (ë³´ì • í›„)',
            note: 'Phase 31 - ëª©í‘œ vs ì‹¤ì œ ì˜¤ë””ì˜¤ ì‹œê°„ ì°¨ì´ ë°˜ì˜'
        });
    }, []); // ì˜ì¡´ì„± ì œê±° - mediaRecorderRef ì‚¬ìš©ìœ¼ë¡œ state ì—…ë°ì´íŠ¸ ì§€ì—° ë¬¸ì œ í•´ê²°

    // ========================================
    // Stop Recording
    // ========================================
    const stopRecording = useCallback(async (
        endTime: number,
        endMeasure: number
    ): Promise<void> => {
        if (!mediaRecorderRef.current || state !== 'recording') {
            return;
        }

        return new Promise((resolve) => {
            const mediaRecorder = mediaRecorderRef.current!;

            mediaRecorder.onstop = async () => {
                console.log('ğŸ¤ [Marker Recording] MediaRecorder stopped, processing...');
                setIsProcessing(true);

                try {
                    // ì¢…ë£Œ ë§ˆì»¤ ê¸°ë¡ (blob ê¸°ì¤€ ìƒëŒ€ ì‹œê°„)
                    const now = performance.now();
                    const endMarker = (now - recordingBlobStartRef.current) / 1000; // ì´ˆ

                    const rawBlob = new Blob(chunksRef.current, {
                        type: mediaRecorder.mimeType
                    });

                    const startTime = pendingRangeRef.current?.startTime || 0;
                    const startMeasure = pendingRangeRef.current?.startMeasure || 1;
                    const rawStartMarker = actualStartMarkerRef.current; // ì¹´ìš´íŠ¸ë‹¤ìš´ ëë‚œ ì‹œì  (blob ê¸°ì¤€)

                    // ========================================
                    // Phase 55: Pull-back (í™©ê¸ˆ ì„¤ì • - íƒ€ì´ë° 83.3%, í”¼ì¹˜ 61.1%)
                    // ê³ ì • ì§€ì—° + ì¶”ê°€ ë²„í¼ë¡œ ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ ì‹±í¬
                    // Phase 75: íƒ€ì´ë° 92.9% ë‹¬ì„± (í™©ê¸ˆ ì„¤ì •)
                    // ========================================
                    const PULLBACK_BUFFER_MS = 250; // ì¶”ê°€ ë²„í¼ (250)
                    const pullbackSeconds = (staticLatencyRef.current + PULLBACK_BUFFER_MS) / 1000;
                    const startMarker = Math.max(0, rawStartMarker - pullbackSeconds);

                    console.log('ğŸ¤ [Phase 55 Pull-back] ì ìš©:', {
                        ì›ë³¸ë§ˆì»¤: rawStartMarker.toFixed(3) + 's',
                        ì¸¡ì •ëœê³ ì •ì§€ì—°: staticLatencyRef.current.toFixed(1) + 'ms',
                        ì¶”ê°€ë²„í¼: PULLBACK_BUFFER_MS + 'ms',
                        ì´ë‹¹ê¹€ëŸ‰: (pullbackSeconds * 1000).toFixed(1) + 'ms',
                        ë³´ì •ë§ˆì»¤: startMarker.toFixed(3) + 's'
                    });

                    console.log('ğŸ¤ [Marker] ë§ˆì»¤ ì •ë³´:', {
                        blobStartTime: recordingBlobStartRef.current.toFixed(0) + 'ms',
                        startMarker: startMarker.toFixed(3) + 's (blob ê¸°ì¤€)',
                        endMarker: endMarker.toFixed(3) + 's (blob ê¸°ì¤€)',
                        extractDuration: (endMarker - startMarker).toFixed(3) + 's',
                        musicTimelineStart: startTime.toFixed(3) + 's',
                        musicTimelineEnd: endTime.toFixed(3) + 's',
                        measures: `${startMeasure}-${endMeasure}`
                    });

                    const segmentId = `seg-${Date.now()}`;

                    // ========================================
                    // ë§ˆì»¤ ê¸°ë°˜ êµ¬ê°„ ì¶”ì¶œ (ë‹¨ìˆœí•˜ê³  ì •í™•í•¨)
                    // ========================================
                    // AudioContext ìƒì„± (ì—†ìœ¼ë©´)
                    if (!audioContextRef.current) {
                        audioContextRef.current = getSharedAudioContext();
                        gainNodeRef.current = audioContextRef.current.createGain();
                        gainNodeRef.current.connect(audioContextRef.current.destination);
                    }

                    const arrayBuffer = await rawBlob.arrayBuffer();
                    const audioBuffer = await audioContextRef.current.decodeAudioData(arrayBuffer);

                    const blobDuration = audioBuffer.duration;
                    console.log('ğŸ¤ [Extract] Blob ì „ì²´ ê¸¸ì´:', blobDuration.toFixed(3) + 's');

                    // startMarker ~ endMarker êµ¬ê°„ë§Œ ì¶”ì¶œ
                    const sampleRate = audioBuffer.sampleRate;
                    const startSample = Math.floor(startMarker * sampleRate);
                    const endSample = Math.floor(endMarker * sampleRate);
                    const extractLength = endSample - startSample;

                    if (extractLength <= 0) {
                        throw new Error('ì¶”ì¶œí•  êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤. startMarker >= endMarker');
                    }

                    console.log('ğŸ¤ [Extract] êµ¬ê°„ ì¶”ì¶œ ì¤€ë¹„:', {
                        startSample,
                        endSample,
                        extractLength,
                        extractDuration: (extractLength / sampleRate).toFixed(3) + 's'
                    });

                    // OfflineAudioContextë¡œ êµ¬ê°„ ì¶”ì¶œ
                    const offlineCtx = new OfflineAudioContext(
                        audioBuffer.numberOfChannels,
                        extractLength,
                        sampleRate
                    );

                    const source = offlineCtx.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(offlineCtx.destination);
                    source.start(0, startMarker, endMarker - startMarker); // startMarkerë¶€í„° (endMarker-startMarker) ê¸¸ì´ë§Œí¼

                    const extractedBuffer = await offlineCtx.startRendering();

                    console.log('ğŸ¤ [Extract] ì¶”ì¶œ ì™„ë£Œ:', {
                        ì›ë³¸ê¸¸ì´: blobDuration.toFixed(3) + 's',
                        ì¶”ì¶œê¸¸ì´: extractedBuffer.duration.toFixed(3) + 's',
                        startMarker: startMarker.toFixed(3) + 's',
                        endMarker: endMarker.toFixed(3) + 's'
                    });

                    // WAVë¡œ ë³€í™˜
                    const wavBlob = audioBufferToWavBlob(extractedBuffer);
                    const url = URL.createObjectURL(wavBlob);

                    // AudioBuffer ì €ì¥ (ì¬ìƒìš©)
                    audioBuffersRef.current.set(segmentId, extractedBuffer);

                    // ========================================
                    // Phase 53: Segment startTimeì—ë„ Pull-back ì ìš©
                    // Pull-backìœ¼ë¡œ blob ì•ë¶€ë¶„ì„ ë” ì¶”ì¶œí•˜ë©´, ê·¸ë§Œí¼ startTimeë„ ì•ë‹¹ê²¨ì•¼
                    // ì¬ìƒ ì‹œ offset ê³„ì‚°ì´ ì •í™•í•´ì§
                    // ========================================
                    const adjustedStartTime = startTime - pullbackSeconds;

                    console.log('ğŸ¤ [Phase 53] Segment ì‹œê°„ ì¡°ì •:', {
                        ì›ë³¸startTime: startTime.toFixed(3) + 's',
                        ì¡°ì •startTime: adjustedStartTime.toFixed(3) + 's',
                        pullback: (pullbackSeconds * 1000).toFixed(1) + 'ms',
                        note: 'ì¬ìƒ ì‹œ offset ê³„ì‚° ì •í™•ë„ í–¥ìƒ'
                    });

                    // Segment ìƒì„±
                    const newSegment: RecordingSegment = {
                        id: segmentId,
                        blob: wavBlob,
                        url,
                        startTime: adjustedStartTime,  // Phase 53: Pull-back ì ìš©
                        endTime,
                        startMeasure,
                        endMeasure
                    };

                    // ê¸°ì¡´ ê²¹ì¹˜ëŠ” segment ì²˜ë¦¬ + ìƒˆ segment ì¶”ê°€
                    setSegments(prev => {
                        const result: RecordingSegment[] = [];

                        for (const seg of prev) {
                            const overlaps = !(endMeasure < seg.startMeasure || startMeasure > seg.endMeasure);

                            if (!overlaps) {
                                result.push(seg);
                            } else {
                                // ê²¹ì¹˜ëŠ” segment ì •ë¦¬
                                console.log('ğŸ¤ Removing overlapped segment:', seg.id, `(${seg.startMeasure}-${seg.endMeasure})`);
                                URL.revokeObjectURL(seg.url);
                                const sourceNode = sourceNodesRef.current.get(seg.id);
                                if (sourceNode) {
                                    try {
                                        sourceNode.stop();
                                        sourceNode.disconnect();
                                    } catch { /* ì´ë¯¸ ì •ì§€ë¨ */ }
                                    sourceNodesRef.current.delete(seg.id);
                                }
                                audioBuffersRef.current.delete(seg.id);
                            }
                        }

                        return [...result, newSegment];
                    });

                    setState('recorded');

                    console.log('ğŸ¤ [Marker Recording] ì™„ë£Œ:', {
                        segmentId,
                        measures: `${startMeasure}-${endMeasure}`,
                        duration: extractedBuffer.duration.toFixed(3) + 's',
                        blobSize: wavBlob.size,
                        note: 'ë§ˆì»¤ ê¸°ë°˜ ì¶”ì¶œ ì™„ë£Œ - preroll ì—†ìŒ'
                    });
                } catch (err) {
                    console.error('Processing error:', err);
                    setError('ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                    onError?.('ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                } finally {
                    setIsProcessing(false);
                    pendingRangeRef.current = null;
                    resolve();
                }
            };

            mediaRecorder.stop();
        });
    }, [state, onError]);

    // ========================================
    // Pause Jamming
    // ========================================
    const pauseJamming = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording' && !isPaused) {
            mediaRecorderRef.current.pause();
            setIsPaused(true);
            console.log('ğŸ¤ Recording paused');
        }
    }, [state, isPaused]);

    // ========================================
    // Resume Jamming
    // ========================================
    const resumeJamming = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording' && isPaused) {
            mediaRecorderRef.current.resume();
            setIsPaused(false);
            console.log('ğŸ¤ Recording resumed');
        }
    }, [state, isPaused]);

    // ========================================
    // Play Recordings At Time (Web Audio API ê¸°ë°˜ - ì •í™•í•œ íƒ€ì´ë°)
    // ========================================
    const playRecordingsAtTime = useCallback(async (fromTime: number) => {
        // Find segments that include this time
        const activeSegments = segments.filter(seg =>
            fromTime >= seg.startTime && fromTime <= seg.endTime
        );

        if (activeSegments.length === 0) {
            return;
        }

        // AudioContext ìƒì„± (ì—†ìœ¼ë©´)
        if (!audioContextRef.current) {
            audioContextRef.current = getSharedAudioContext();
            gainNodeRef.current = audioContextRef.current.createGain();
            gainNodeRef.current.connect(audioContextRef.current.destination);
        }

        const context = audioContextRef.current;

        // suspended ìƒíƒœë©´ resume (ë°˜ë“œì‹œ await!)
        if (context.state === 'suspended') {
            console.log('ğŸ¤ [Web Audio] Resuming suspended AudioContext');
            await context.resume();
        }

        console.log('ğŸ¤ [Web Audio] AudioContext state:', context.state, 'gainNode:', gainNodeRef.current?.gain.value);

        // AudioContext.currentTime ê¸°ë°˜ ì¦‰ì‹œ ì¬ìƒ (ë°°ê²½ìŒì•…ê³¼ ì •í™•íˆ ë™ê¸°í™”)
        const scheduleTime = context.currentTime; // ì¦‰ì‹œ ì¬ìƒ (0ms lookahead)

        // Play each active segment using Web Audio API
        activeSegments.forEach(seg => {
            const audioBuffer = audioBuffersRef.current.get(seg.id);
            if (!audioBuffer) {
                console.warn('ğŸ¤ AudioBuffer not ready for segment:', seg.id);
                return;
            }

            // ê¸°ì¡´ source node ì •ë¦¬
            const existingSource = sourceNodesRef.current.get(seg.id);
            if (existingSource) {
                try {
                    existingSource.stop();
                    existingSource.disconnect();
                } catch {
                    // ì´ë¯¸ ì •ì§€ë¨
                }
            }

            // ë¸”ë¡­ ë‚´ ì˜¤í”„ì…‹ ê³„ì‚° (ë§ˆì»¤ ê¸°ë°˜ - preroll ì—†ìŒ)
            // fromTime - seg.startTime: ê³¡ ì‹œê°„ ë‚´ ì˜¤í”„ì…‹
            const offset = Math.max(0, fromTime - seg.startTime);

            // ìƒˆ source node ìƒì„±
            const source = context.createBufferSource();
            source.buffer = audioBuffer;
            if (gainNodeRef.current) {
                source.connect(gainNodeRef.current);
            } else {
                source.connect(context.destination);
            }

            // AudioContext.currentTime ê¸°ë°˜ ì •í™•í•œ ìŠ¤ì¼€ì¤„ë§ (ë°°ê²½ìŒì•…ê³¼ ë™ê¸°í™”)
            source.start(scheduleTime, offset);
            sourceNodesRef.current.set(seg.id, source);

            console.log('ğŸ¤ [Web Audio] ì¬ìƒ ì‹œì‘ (ìŠ¤ì¼€ì¤„ë§):', {
                segId: seg.id,
                fromTime,
                segStartTime: seg.startTime,
                offset: offset.toFixed(3),
                scheduleTime: scheduleTime.toFixed(3),
                bufferDuration: audioBuffer.duration.toFixed(2)
            });

            // ì¬ìƒ ì™„ë£Œ ì‹œ ì •ë¦¬
            source.onended = () => {
                sourceNodesRef.current.delete(seg.id);
            };
        });
    }, [segments]);

    // ========================================
    // Pause Recordings (Web Audio API ê¸°ë°˜)
    // ========================================
    const pauseRecordings = useCallback(() => {
        const count = sourceNodesRef.current.size;
        console.log('ğŸ¤ [pauseRecordings] í˜¸ì¶œë¨, í™œì„± ì†ŒìŠ¤:', count);

        sourceNodesRef.current.forEach((source, id) => {
            try {
                source.stop();
                source.disconnect();
                console.log('ğŸ¤ [pauseRecordings] ì†ŒìŠ¤ ì •ì§€:', id);
            } catch (e) {
                console.log('ğŸ¤ [pauseRecordings] ì†ŒìŠ¤ ì´ë¯¸ ì •ì§€ë¨:', id);
            }
        });
        sourceNodesRef.current.clear();
        console.log('ğŸ¤ [pauseRecordings] ì™„ë£Œ, ë‚¨ì€ ì†ŒìŠ¤:', sourceNodesRef.current.size);
    }, []);

    // ========================================
    // Reset Recording
    // ========================================
    const resetRecording = useCallback(() => {
        if (mediaRecorderRef.current && state === 'recording') {
            mediaRecorderRef.current.stop();
        }

        // Clean up Web Audio API
        sourceNodesRef.current.forEach(source => {
            try {
                source.stop();
                source.disconnect();
            } catch {
                // ì´ë¯¸ ì •ì§€ë¨
            }
        });
        sourceNodesRef.current.clear();
        audioBuffersRef.current.clear();

        // Revoke all URLs
        segments.forEach(seg => URL.revokeObjectURL(seg.url));

        // Reset state
        setSegments([]);
        setState('idle');
        setIsPaused(false);
        setError(null);
        chunksRef.current = [];
        pendingRangeRef.current = null;

        console.log('ğŸ¤ All recordings reset');
    }, [state, segments]);

    return {
        state,
        permissionState,
        segments,
        recordedMeasures,
        isProcessing,
        isPaused,
        error,
        audioBlob,
        recordingRange,
        // Phase 52: Warm-up ìƒíƒœ
        isWarmedUp,
        measuredLatency,
        requestPermission,
        // Phase 52: Warm-up í•¨ìˆ˜
        warmUp,
        startRecording,
        markActualStart,
        stopRecording,
        pauseJamming,
        resumeJamming,
        playRecordingsAtTime,
        pauseRecordings,
        resetRecording,
        hasRecordingAt,
        getOverlappingSegment
    };
}

export default useRecorder;
